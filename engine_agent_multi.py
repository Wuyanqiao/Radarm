"""
å¤šä¸“å®¶æ··åˆ Agent å¼•æ“ï¼ˆåº•å±‚èƒ½åŠ›ï¼‰
------------------------------
ç”¨äºâ€œç”¨æˆ·èŠå¤© Radarm AI agent çš„å¤šä¸“å®¶æ··åˆæ¨¡å¼â€ã€‚

ä¸‰è§’è‰²é—­ç¯ï¼š
- Plannerï¼ˆæ¶æ„å¸ˆï¼‰ï¼šç”Ÿæˆåˆ†æè“å›¾ï¼ˆä¸å†™ä»£ç ï¼‰
- Executorï¼ˆç¨‹åºå‘˜ï¼‰ï¼šæ ¹æ®è“å›¾å†™ Python ä»£ç 
- Verifierï¼ˆè¯„å®¡ï¼‰ï¼šåŸºäºæ‰§è¡Œç»“æœåˆ¤å®š PASS/FAILï¼Œå¹¶ç»™å‡ºä¿®å¤å»ºè®®

æ”¯æŒæœ‰é™è½®æ¬¡è¿­ä»£ï¼šFAIL -> å¸¦åé¦ˆé‡è¯•ã€‚
"""

import json
import re
import importlib
from typing import Any, Dict, List, Optional

PROMPTS = {
    "planner": """
ä½ æ˜¯ä¸€åã€æ•°æ®åˆ†ææ¶æ„å¸ˆã€‘ï¼ˆåç»Ÿè®¡å»ºæ¨¡ï¼‰ã€‚
è¿è¡Œç¯å¢ƒè¯´æ˜ï¼šç³»ç»Ÿå·²æä¾› Pandas DataFrame `df`ï¼ˆå†…å­˜æ•°æ®ï¼‰ï¼Œä¸¥ç¦è¯»å–ä»»ä½•å¤–éƒ¨æ–‡ä»¶/ç½‘ç»œï¼ˆä¸è¦ read_csv/read_excelï¼Œä¸è¦ data.csvï¼‰ã€‚

ç”¨æˆ·é—®é¢˜ï¼š{user_query}
æ•°æ®æ¦‚å†µï¼š{data_context}
{feedback_context}

ã€é‡è¦æç¤º - å›¾ç‰‡å’Œè§†è§‰ç†è§£æ•°æ®ã€‘
å¦‚æœä¸Šé¢çš„"æ•°æ®æ¦‚å†µ"ä¸­åŒ…å«"[è§†è§‰ç†è§£]"æˆ–"[å›¾ç‰‡é™„ä»¶]"éƒ¨åˆ†ï¼š
1. **å®Œæ•´ç†è§£å›¾ç‰‡ä¿¡æ¯**ï¼šä»”ç»†é˜…è¯»è§†è§‰ç†è§£ç»“æœï¼Œå…¨é¢ç†è§£å›¾ç‰‡ä¸­çš„æ‰€æœ‰ä¿¡æ¯ï¼ˆæ–‡å­—ã€è¡¨æ ¼ã€å›¾è¡¨ã€æ ‡å‡†ã€è§„èŒƒã€ç•Œé¢å…ƒç´ ã€å›¾åƒå†…å®¹ç­‰ï¼‰
2. **åœ¨è§„åˆ’ä¸­ä½“ç°å›¾ç‰‡ä¿¡æ¯**ï¼šå¦‚æœç”¨æˆ·é—®é¢˜æ¶‰åŠå›¾ç‰‡ä¸­çš„ä»»ä½•ä¿¡æ¯ï¼ˆæ ‡å‡†ã€è§„èŒƒã€æ•°æ®ã€æ–‡å­—è¯´æ˜ã€å›¾è¡¨è¶‹åŠ¿ç­‰ï¼‰ï¼Œ**å¿…é¡»åœ¨è§„åˆ’ä¸­æ˜ç¡®æŒ‡å‡ºè¿™äº›ä¿¡æ¯ä»¥åŠå¦‚ä½•åœ¨åˆ†æä¸­ä½¿ç”¨å®ƒä»¬**
3. **å……åˆ†åˆ©ç”¨å›¾ç‰‡å†…å®¹**ï¼šç¡®ä¿è§„åˆ’çš„åˆ†æç­–ç•¥èƒ½å¤Ÿå……åˆ†åˆ©ç”¨å›¾ç‰‡ä¸­çš„ç›¸å…³ä¿¡æ¯ï¼Œæ— è®ºæ˜¯ç”¨äºåˆ¤å®šã€åˆ†ç±»ã€è®¡ç®—ã€éªŒè¯è¿˜æ˜¯å…¶ä»–ç”¨é€”
4. **ä¿¡æ¯æå–ç­–ç•¥**ï¼šå¦‚æœå›¾ç‰‡åŒ…å«ç»“æ„åŒ–ä¿¡æ¯ï¼ˆè¡¨æ ¼ã€æ ‡å‡†ç­‰ï¼‰ï¼Œåº”åœ¨è§„åˆ’ä¸­æ˜ç¡®è¯´æ˜å¦‚ä½•æå–å’Œåº”ç”¨è¿™äº›ä¿¡æ¯
5. **å…¨é¢è€ƒè™‘**ï¼šä¸è¦åªå…³æ³¨ç‰¹å®šç±»å‹çš„ä¿¡æ¯ï¼Œè¦å…¨é¢è€ƒè™‘å›¾ç‰‡ä¸­çš„æ‰€æœ‰å†…å®¹å¯¹åˆ†æä»»åŠ¡çš„ä»·å€¼

è¯·è¾“å‡º"ç²¾ç®€ä½†å¯æ‰§è¡Œ"çš„åˆ†æè“å›¾ï¼ˆä¸è¦å†™ä»£ç ï¼‰ï¼ŒåŒ…å«ï¼š
1) ç›®æ ‡å˜é‡/å…³é”®è‡ªå˜é‡ï¼ˆä»æ•°æ®æ¦‚å†µæ¨æ–­å¯èƒ½åˆ—åï¼Œå¿…è¦æ—¶åˆ—å‡ºå€™é€‰æ˜ å°„ï¼‰
2) é¢„å¤„ç†ç­–ç•¥ï¼ˆç¼ºå¤±ã€ç±»å‹ã€å¼‚å¸¸å€¼ã€æ´¾ç”Ÿå˜é‡å¦‚ BMI=ä½“é‡/(èº«é«˜^2) çš„æ¡ä»¶ï¼‰
3) å»ºæ¨¡ä¸æ˜¾è‘—æ€§æ£€éªŒï¼ˆä¼˜å…ˆï¼šç›¸å…³ + å¤šå…ƒçº¿æ€§å›å½’/å¹¿ä¹‰çº¿æ€§ï¼›è¯´æ˜è¦è¾“å‡ºçš„ p å€¼/ç½®ä¿¡åŒºé—´/RÂ²/æ ·æœ¬é‡ï¼‰
4) è¯Šæ–­ä¸ä¸‹ä¸€æ­¥ï¼ˆå…±çº¿æ€§ã€æ®‹å·®ã€æ•æ„Ÿæ€§åˆ†æï¼‰
è¦æ±‚ï¼šæ¡ç›®åŒ–è¾“å‡ºï¼Œæœ€å¤š 8 æ¡ï¼›é¿å…ç©ºè¯ã€‚
""",
    "executor": """
ä½ æ˜¯ä¸€åã€å»ºæ¨¡ç¨‹åºå‘˜ã€‘ï¼ˆPython/Pandasï¼‰ã€‚
è¿è¡Œç¯å¢ƒè¯´æ˜ï¼šç³»ç»Ÿå·²æä¾› DataFrame `df`ï¼ˆå†…å­˜æ•°æ®ï¼‰ï¼Œä¸¥ç¦è¯»å–ä»»ä½•å¤–éƒ¨æ–‡ä»¶/ç½‘ç»œï¼š
- ä¸è¦ pd.read_csv/read_excel/read_*ï¼Œä¸è¦ä½¿ç”¨ data.csv
- ä¸è¦ open()/os/pathlib/requests/socket/subprocess
æç¤ºï¼šä½ å¯ä»¥ä½¿ç”¨ç³»ç»Ÿé¢„ç½®çš„è¾…åŠ©å‡½æ•° `find_col('å€™é€‰1','å€™é€‰2',...)` æ¥åšåˆ—åæ¨¡ç³ŠåŒ¹é…ï¼ˆè¿”å›çœŸå®åˆ—åæˆ– Noneï¼‰ã€‚

ã€æ¶æ„å¸ˆè“å›¾ã€‘
{plan}

ã€æ•°æ®æ¦‚å†µã€‘
{data_context}

ã€é‡è¦æç¤º - å›¾ç‰‡å’Œè§†è§‰ç†è§£æ•°æ®ã€‘
å¦‚æœä¸Šé¢çš„"æ•°æ®æ¦‚å†µ"ä¸­åŒ…å«"[è§†è§‰ç†è§£]"æˆ–"[å›¾ç‰‡é™„ä»¶]"éƒ¨åˆ†ï¼š
1. **å®Œæ•´ç†è§£å›¾ç‰‡ä¿¡æ¯**ï¼šä»”ç»†é˜…è¯»è§†è§‰ç†è§£ç»“æœï¼Œç†è§£å›¾ç‰‡ä¸­çš„æ‰€æœ‰ä¿¡æ¯ï¼ˆæ–‡å­—ã€è¡¨æ ¼ã€å›¾è¡¨ã€æ ‡å‡†ã€è§„èŒƒã€ç•Œé¢å…ƒç´ ã€å›¾åƒå†…å®¹ç­‰ï¼‰
2. **æå–å¹¶ä½¿ç”¨å›¾ç‰‡ä¿¡æ¯**ï¼šæ ¹æ®æ¶æ„å¸ˆè“å›¾å’Œè§†è§‰ç†è§£ç»“æœï¼Œæå–å›¾ç‰‡ä¸­çš„ä»»ä½•ç›¸å…³ä¿¡æ¯å¹¶åœ¨ä»£ç ä¸­ä½¿ç”¨
3. **ç»“æ„åŒ–æ•°æ®å®šä¹‰**ï¼šå¦‚æœå›¾ç‰‡åŒ…å«è¡¨æ ¼ã€æ ‡å‡†ã€è§„èŒƒã€é™å€¼ç­‰ç»“æ„åŒ–ä¿¡æ¯ï¼Œä¸”ä»£ç ä¸­éœ€è¦ä½¿ç”¨è¿™äº›ä¿¡æ¯ï¼Œ**å¿…é¡»åœ¨ä»£ç å¼€å¤´å…ˆè§£æå¹¶å®šä¹‰ç›¸åº”çš„æ•°æ®ç»“æ„**ï¼ˆå¦‚å­—å…¸ã€DataFrameã€åˆ—è¡¨ç­‰ï¼‰
4. **ç¤ºä¾‹**ï¼š
   - å¦‚æœè§†è§‰ç†è§£æåˆ°æ ‡å‡†é™å€¼ï¼ˆå¦‚"æ€»é…¸â‰¥0.4ï¼ˆä¼˜çº§ï¼‰"ï¼‰ï¼Œåº”åˆ›å»ºç±»ä¼¼ `standards = {{'æ€»é…¸': {{'ä¼˜çº§': 0.4, 'ä¸€çº§': 0.3}}}}` çš„ç»“æ„
   - å¦‚æœè§†è§‰ç†è§£æåˆ°è¡¨æ ¼æ•°æ®ï¼Œåº”åˆ›å»ºç›¸åº”çš„DataFrameæˆ–å­—å…¸ç»“æ„
   - å¦‚æœè§†è§‰ç†è§£æåˆ°å…¶ä»–ç»“æ„åŒ–ä¿¡æ¯ï¼Œåº”æ ¹æ®éœ€è¦åˆ›å»ºç›¸åº”çš„æ•°æ®ç»“æ„
5. **é¿å…ç¡¬ç¼–ç **ï¼šç¡®ä¿ä»£ç ä¸­ä½¿ç”¨çš„å›¾ç‰‡ä¿¡æ¯éƒ½ä»è§†è§‰ç†è§£ç»“æœä¸­æå–å¹¶å®šä¹‰ï¼Œè€Œä¸æ˜¯ç›´æ¥ç¡¬ç¼–ç æˆ–å¼•ç”¨æœªå®šä¹‰çš„å˜é‡
6. **å……åˆ†åˆ©ç”¨æ‰€æœ‰ä¿¡æ¯**ï¼šä¸è¦åªå…³æ³¨è¡¨æ ¼æˆ–æ ‡å‡†ï¼Œè¦å……åˆ†åˆ©ç”¨å›¾ç‰‡ä¸­çš„ä»»ä½•ç›¸å…³ä¿¡æ¯ï¼ˆæ–‡å­—è¯´æ˜ã€å›¾è¡¨è¶‹åŠ¿ã€ç•Œé¢çŠ¶æ€ã€å›¾åƒç‰¹å¾ç­‰ï¼‰

ã€å·¥å…·ç®±ã€‘
- pandas / numpy / matplotlib / seaborn
- **å›å½’æ¨¡æ¿å‡½æ•°**ï¼š`fit_linear_regression(y, X, feature_names=None)` - è‡ªåŠ¨è®¡ç®—ç³»æ•°ã€på€¼ã€RÂ²ã€ç½®ä¿¡åŒºé—´ï¼ˆä¸ä¾èµ– statsmodelsï¼‰
- scipy.statsï¼šç”¨äºå…¶ä»–ç»Ÿè®¡æ£€éªŒï¼ˆtæ£€éªŒã€ANOVAã€ç›¸å…³ç­‰ï¼‰
- æœºå™¨å­¦ä¹ ï¼šml.run(df, target='...', task='regression'/'classification'/'clustering', k=...)

ã€ä»»åŠ¡ã€‘
ç¼–å†™ Python ä»£ç å®ç°è“å›¾ã€‚è¦æ±‚ï¼š
1) åªèƒ½ä½¿ç”¨å·²å­˜åœ¨çš„ dfï¼ˆä¸è¦åŠ è½½æ•°æ®ï¼‰
2) å¤„ç†ç¼ºå¤±å€¼ä¸ç±»å‹è½¬æ¢ï¼šåœ¨å»ºæ¨¡å‰æŠŠç›¸å…³åˆ—è½¬ä¸ºæ•°å€¼ï¼ŒæŠ¥å‘Šæœ‰æ•ˆæ ·æœ¬é‡ n
3) è‹¥ç”¨æˆ·æåˆ° BMI ä½†æ•°æ®æ²¡æœ‰ BMI åˆ—ï¼šå°è¯•ç”¨"èº«é«˜/ä½“é‡"æ¨æ–­å¹¶è®¡ç®—ï¼›æ— æ³•æ¨æ–­æ—¶å¿…é¡»åœ¨ result ä¸­è¯´æ˜ç¼ºå¤±å­—æ®µ
4) å»ºç«‹å…³ç³»æ¨¡å‹å¹¶æ£€éªŒæ˜¾è‘—æ€§ï¼š**å¼ºçƒˆå»ºè®®ä½¿ç”¨ `fit_linear_regression(y, X, feature_names)`**ï¼Œå®ƒä¼šè‡ªåŠ¨è¾“å‡ºç³»æ•°ã€på€¼ã€RÂ²ã€ç½®ä¿¡åŒºé—´ç­‰å®Œæ•´ç»“æœï¼ˆè¿”å› dictï¼Œå¯ç”¨ result = reg_result['summary'] è·å–æ ¼å¼åŒ–æ–‡æœ¬ï¼‰
5) æœ€ç»ˆç»“è®ºå¿…é¡»èµ‹å€¼ç»™å˜é‡ resultï¼ˆå»ºè®®æ˜¯ Markdown æ–‡æœ¬ï¼ŒåŒ…å«ç»“è®º+æ˜¾è‘—æ€§+ä¸‹ä¸€æ­¥ï¼‰
6) ç»˜å›¾ä¸è¦ plt.show()ï¼ˆå¯é€‰ç”»æ•£ç‚¹+æ‹Ÿåˆçº¿ï¼‰
7) å°½é‡ä¸è¦ print è¿‡å¤šå†…å®¹ï¼ˆç³»ç»Ÿä¼šæŠŠ print å½“ä½œæœ€ç»ˆè¾“å‡ºï¼‰

ã€å»ºè®®è¾“å‡ºæ¨¡æ¿ã€‘
è¯·å°† result ç»„ç»‡ä¸ºä¸­æ–‡ Markdownï¼Œè‡³å°‘åŒ…å«ï¼š
- ç›®æ ‡å˜é‡ã€ä¸»è¦è‡ªå˜é‡ï¼ˆå­•å‘¨ã€BMI ç­‰ï¼‰ã€æ§åˆ¶å˜é‡ï¼ˆå¯é€‰ï¼‰
- æ¨¡å‹æ–¹æ³•ï¼ˆä¾‹å¦‚ OLS + ç¨³å¥æ ‡å‡†è¯¯ï¼‰
- å…³é”®ç³»æ•°ä¸ p å€¼ï¼ˆé‡ç‚¹è§£é‡Šå­•å‘¨/BMIï¼‰
- æ ·æœ¬é‡ nã€RÂ²/Adj.RÂ²ï¼ˆæˆ–è¿‘ä¼¼æŒ‡æ ‡ï¼‰
- ç»“è®ºä¸ä¸‹ä¸€æ­¥ï¼ˆè‹¥å­—æ®µä¸è¶³åˆ™æ˜ç¡®ç¼ºä»€ä¹ˆï¼‰

åªè¾“å‡ºä¸€ä¸ª ```python ä»£ç å—ï¼Œä¸è¦è§£é‡Šã€‚
""",
    "verifier": """
ä½ æ˜¯ä¸€åã€å»ºæ¨¡è¯„å®¡ã€‘ï¼ˆä¸¥æ ¼ï¼‰ã€‚

ã€æ¶æ„å¸ˆè“å›¾ã€‘
{plan}

ã€ç¨‹åºå‘˜ä»£ç ã€‘
{code}

ã€è¿è¡Œç»“æœã€‘
{execution_result}

å®¡æŸ¥çº¢çº¿ï¼ˆè¿åä»»ä¸€æ¡å¿…é¡» FAILï¼‰ï¼š
1) ä»£ç æŠ¥é”™ï¼ˆå‡ºç° Error / Traceback / Exceptionï¼‰
2) ç»“æœä¸ºç©ºï¼ˆNone æˆ– ç©ºå­—ç¬¦ä¸²ï¼‰

è¯·åªè¾“å‡ºæ ‡å‡† JSONï¼ˆä¸è¦ Markdown/ä¸è¦ä»£ç å—/ä¸è¦é¢å¤–å‰åç¼€ï¼‰ï¼Œå¿…é¡»èƒ½è¢« json.loads è§£æï¼š
{{
  "status": "PASS" æˆ– "FAIL",
  "reason": "é€šè¿‡ç†ç”±æˆ–å¤±è´¥åŸå› ï¼ˆå°½é‡å…·ä½“ï¼‰",
  "suggestion": "å¦‚æœ FAILï¼Œç»™å‡ºå¯æ‰§è¡Œçš„ä¿®å¤å»ºè®®ï¼ˆå¿…è¦æ—¶æŒ‡å‡ºåº”æ”¹å“ªå‡ è¡Œ/æ”¹ä»€ä¹ˆï¼‰",
  "final_reply": "å¦‚æœ PASSï¼šç»™ç”¨æˆ·çš„æœ€ç»ˆç­”å¤ï¼ˆä¸­æ–‡ï¼Œå¿…é¡»åŸºäºè¿è¡Œç»“æœï¼Œä¸è¦æœæ’°æ•°å­—ï¼›è‹¥ç¼ºå°‘å…³é”®è¾“å‡ºå°±æŒ‡å‡ºå¹¶ç»™ä¸‹ä¸€æ­¥ï¼‰"
}}
""",
}


def _call_llm(provider: str, api_key: str, model_config: Dict[str, Any], prompt: str) -> str:
    if not api_key:
        return f"Error: ç¼ºå°‘ {provider} Key"
    cfg = model_config.get(provider)
    if not cfg:
        return f"Error: æœªçŸ¥æ¨¡å‹ provider={provider}"

    try:
        requests = importlib.import_module("requests")
    except Exception:
        return "Error: ç¼ºå°‘ requests ä¾èµ–"

    headers = {"Content-Type": "application/json", "Authorization": f"Bearer {api_key}"}
    payload = {"model": cfg["model"], "messages": [{"role": "user", "content": prompt}], "temperature": 0.1}
    try:
        resp = requests.post(cfg["url"], headers=headers, json=payload, timeout=120)
        if resp.status_code != 200:
            return f"Error: {resp.status_code} {resp.text}"
        return resp.json()["choices"][0]["message"]["content"]
    except Exception as e:
        return f"Error: {str(e)}"


def _extract_python_code(text: str) -> str:
    m = re.search(r"```python(.*?)```", text, re.DOTALL | re.IGNORECASE)
    return m.group(1).strip() if m else text.strip()


def _extract_json(text: str) -> Optional[Dict[str, Any]]:
    if not text:
        return None
    s = str(text).strip()
    if not s:
        return None
    # 1) strict json
    try:
        obj = json.loads(s)
        return obj if isinstance(obj, dict) else None
    except Exception:
        pass
    # 2) scan first json object in noisy text
    dec = json.JSONDecoder()
    for m in re.finditer(r"\{", s):
        try:
            obj, _end = dec.raw_decode(s[m.start() :])
            return obj if isinstance(obj, dict) else None
        except Exception:
            continue
    # 3) regex fallback
    try:
        m = re.search(r"\{[\s\S]*\}", s)
        if not m:
            return None
        obj = json.loads(m.group(0))
        return obj if isinstance(obj, dict) else None
    except Exception:
        return None


def _norm_name(s: str) -> str:
    return re.sub(r"[\s\-_]+", "", str(s or "").strip().lower())


def _pick_first(columns: List[str], candidates: List[str]) -> Optional[str]:
    cols = list(columns or [])
    norm_map = {_norm_name(c): c for c in cols}
    # 1) exact/normalized match
    for cand in candidates or []:
        cn = _norm_name(cand)
        if cn in norm_map:
            return norm_map[cn]
    # 2) substring match
    for cand in candidates or []:
        cn = _norm_name(cand)
        if not cn:
            continue
        for c in cols:
            if cn in _norm_name(c):
                return c
    return None


def _collect_contains(columns: List[str], keywords: List[str]) -> List[str]:
    out: List[str] = []
    for c in columns or []:
        n = _norm_name(c)
        if any(_norm_name(k) in n for k in keywords or []):
            out.append(c)
    # å»é‡ä¿åº
    seen = set()
    res = []
    for c in out:
        if c not in seen:
            res.append(c)
            seen.add(c)
    return res


def build_semantic_hints(columns: List[str]) -> Dict[str, Any]:
    """
    åŸºäºåˆ—åç»™å‡ºâ€œè¯­ä¹‰å€™é€‰æ˜ å°„â€ï¼Œç”¨äºå¸®åŠ© LLM æ›´ç¨³å¥åœ°é€‰åˆ—/å»ºæ¨¡ã€‚
    """
    cols = [str(c) for c in (columns or [])]

    core = {
        "age": _pick_first(cols, ["å¹´é¾„", "age"]),
        "height": _pick_first(cols, ["èº«é«˜", "height"]),
        "weight": _pick_first(cols, ["ä½“é‡", "weight"]),
        "bmi": _pick_first(cols, ["å­•å¦‡BMI", "BMI", "ä½“è´¨æŒ‡æ•°"]),
        "gestational_week": _pick_first(cols, ["æ£€æµ‹å­•å‘¨", "å­•å‘¨", "gest_week", "ga"]),
        "lmp": _pick_first(cols, ["æœ«æ¬¡æœˆç»", "LMP"]),
        "test_date": _pick_first(cols, ["æ£€æµ‹æ—¥æœŸ", "æŠ½è¡€æ—¥æœŸ", "æ—¥æœŸ"]),
        "ivf": _pick_first(cols, ["IVFå¦Šå¨ ", "ivf"]),
        "y_conc": _pick_first(cols, ["YæŸ“è‰²ä½“æµ“åº¦", "èƒå„¿YæŸ“è‰²ä½“æµ“åº¦", "Yæµ“åº¦", "LYæŸ“è‰²ä½“æµ“åº¦"]),
        "x_conc": _pick_first(cols, ["XæŸ“è‰²ä½“æµ“åº¦", "Xæµ“åº¦"]),
        "y_z": _pick_first(cols, ["YæŸ“è‰²ä½“çš„Zå€¼", "YæŸ“è‰²ä½“Zå€¼", "Y Zå€¼"]),
        "x_z": _pick_first(cols, ["XæŸ“è‰²ä½“çš„Zå€¼", "XæŸ“è‰²ä½“Zå€¼", "X Zå€¼"]),
        "aneuploidy": _pick_first(cols, ["æŸ“è‰²ä½“çš„éæ•´å€ä½“", "éæ•´å€ä½“"]),
        "fetal_health": _pick_first(cols, ["èƒå„¿æ˜¯å¦å¥åº·", "æ˜¯å¦å¥åº·"]),
    }

    chrom_z = {}
    for c in cols:
        m = re.search(r"([0-9]{1,2}|X|Y)å·?æŸ“è‰²ä½“çš„Zå€¼", c)
        if m:
            chrom_z[m.group(1)] = c

    qc_covariates = _collect_contains(
        cols,
        [
            "åŸå§‹è¯»æ®µ",
            "æ¯”å¯¹",
            "é‡å¤è¯»æ®µ",
            "å”¯ä¸€æ¯”å¯¹",
            "GCå«é‡",
            "è¢«è¿‡æ»¤",
        ],
    )

    # å¸¸è§åˆ†ç±»/äºŒå…ƒå­—æ®µï¼ˆå¯èƒ½éœ€è¦ one-hotï¼‰
    categorical_candidates = _collect_contains(cols, ["æ˜¯å¦", "å¦Šå¨ ", "éæ•´å€ä½“"])

    notes: List[str] = []
    if core.get("bmi") is None and core.get("height") and core.get("weight"):
        notes.append("BMI åˆ—ç¼ºå¤±ï¼šå¯å°è¯•ç”¨ ä½“é‡/(èº«é«˜^2) è®¡ç®—ï¼ˆæ³¨æ„èº«é«˜å•ä½ cm/mï¼‰ã€‚")
    if core.get("gestational_week") is None and core.get("lmp") and core.get("test_date"):
        notes.append("å­•å‘¨åˆ—ç¼ºå¤±ï¼šå¯å°è¯•ç”¨ (æ£€æµ‹æ—¥æœŸ-æœ«æ¬¡æœˆç»)/7 è®¡ç®—å­•å‘¨ï¼ˆéœ€æ—¥æœŸå¯è§£æï¼‰ã€‚")

    return {
        "core_columns": core,
        "qc_covariates": qc_covariates,
        "chromosome_z_scores": chrom_z,
        "categorical_candidates": categorical_candidates,
        "notes": notes,
    }


def _auto_feedback_from_exec(exec_text: Any) -> str:
    t = str(exec_text or "")
    if not t:
        return ""
    if "No such file or directory" in t or "FileNotFoundError" in t:
        return "ä¸è¦è¯»å–ä»»ä½•æœ¬åœ°æ–‡ä»¶ï¼ˆå¦‚ data.csvï¼‰ã€‚è¿è¡Œç¯å¢ƒå·²æä¾› dfï¼Œè¯·ç›´æ¥ä½¿ç”¨ df è¿›è¡Œåˆ†æã€‚"
    if "KeyError" in t:
        return "å‡ºç° KeyErrorï¼ˆåˆ—åä¸å­˜åœ¨ï¼‰ã€‚è¯·ç”¨ df.columns æ£€æŸ¥çœŸå®åˆ—åï¼Œåšæ¨¡ç³ŠåŒ¹é…/å€™é€‰æ˜ å°„ï¼Œå¹¶åœ¨å»ºæ¨¡å‰ç»Ÿä¸€é‡å‘½åã€‚"
    if "ModuleNotFoundError" in t:
        return "ç¯å¢ƒç¼ºå°‘æŸäº›ç¬¬ä¸‰æ–¹åº“ã€‚è¯·é¿å…ä½¿ç”¨ä¸å¯ç”¨åº“ï¼Œæˆ–å¯¹ import åš try/except å¹¶æä¾› pandas/numpy çš„æ›¿ä»£å®ç°ã€‚"
    if "ç¦æ­¢æ–‡ä»¶/ç½‘ç»œ/ç³»ç»Ÿæ“ä½œ" in t:
        return "ç³»ç»Ÿç¦æ­¢æ–‡ä»¶/ç½‘ç»œ/ç³»ç»Ÿæ“ä½œã€‚è¯·ç§»é™¤ read_csv/read_excel/open/requests ç­‰ï¼Œç›´æ¥ä½¿ç”¨ df è¿›è¡Œè®¡ç®—ã€‚"
    return ""


def _provider_label(provider: str) -> str:
    """
    ä»…ç”¨äº UI/æ—¥å¿—å±•ç¤ºï¼šæŠŠå†…éƒ¨ provider id æ˜ å°„ä¸ºç”¨æˆ·å¯ç†è§£çš„ DeepSeek æ§½ä½åã€‚
    """
    mapping = {
        "deepseekA": "DeepSeek-A",
        "deepseekB": "DeepSeek-B",
        "deepseekC": "DeepSeek-C",
        "zhipu": "Zhipu",
        "qwen": "Qwen",
    }
    return mapping.get(str(provider), str(provider))


def run_multi_agent_engine(
    *,
    user_query: str,
    data_context: str,
    api_keys: Dict[str, str],
    model_config: Dict[str, Any],
    roles: Optional[Dict[str, str]] = None,
    execute_callback,
    df,
) -> Dict[str, Any]:
    """
    å¤šä¸“å®¶æ··åˆ Agent å¼•æ“å…¥å£ï¼ˆä¾› workflow_multi_chat.py è°ƒç”¨ï¼‰
    """
    roles = roles or {"planner": "deepseekA", "executor": "deepseekB", "verifier": "deepseekC"}

    # Key æ£€æŸ¥ä¸è‡ªåŠ¨è¡¥ä½ï¼ˆå…è®¸åªé… 1 ä¸ª keyï¼Œä½†æ•ˆæœä¼šä¸‹é™ï¼‰
    available_keys = [k for k, v in api_keys.items() if v]
    if not available_keys:
        return {"error": "æœªé…ç½® API Key", "process_log": "âŒ æ—  Key"}
    for r in roles:
        if not api_keys.get(roles[r]):
            roles[r] = available_keys[0]

    process_log: List[str] = []
    iteration = 0
    max_iterations = 2
    feedback = ""

    while iteration < max_iterations:
        iter_prefix = f"#### [ç¬¬ {iteration + 1} è½®è¿­ä»£]"
        feedback_context = f"\n\n[ä¸Šä¸€è½®åé¦ˆ]\n{feedback}\n" if feedback else ""

        # ç»™æ¨¡å‹æä¾›â€œåˆ—è¯­ä¹‰å€™é€‰æ˜ å°„â€ï¼Œæå‡é€‰åˆ—ä¸å»ºæ¨¡å‘½ä¸­ç‡
        try:
            hints = build_semantic_hints([str(c) for c in getattr(df, "columns", [])])
            hints_text = json.dumps(hints, ensure_ascii=False, indent=2)
        except Exception:
            hints_text = "{}"
        enriched_data_context = (
            (data_context or "")
            + "\n\n[å­—æ®µå€™é€‰æ˜ å°„(JSON)]\n"
            + hints_text
            + "\n\nè¯·ä¼˜å…ˆä½¿ç”¨ä¸Šè¿° core_columns/qc_covariates ä¸­çš„çœŸå®åˆ—åï¼›è‹¥ç”¨æˆ·æåˆ°çš„æ¦‚å¿µæ— å¯¹åº”åˆ—ï¼Œå¿…é¡»åœ¨ç»“è®ºä¸­è¯´æ˜å¹¶ç»™å‡ºè¡¥å……å­—æ®µå»ºè®®ã€‚"
        )

        # 1) è§„åˆ’
        process_log.append(f"{iter_prefix}\n**ğŸ§  æ¶æ„å¸ˆ ({_provider_label(roles['planner'])}) æ­£åœ¨è§„åˆ’...**")
        plan = _call_llm(
            roles["planner"],
            api_keys[roles["planner"]],
            model_config,
            PROMPTS["planner"].format(user_query=user_query, data_context=enriched_data_context, feedback_context=feedback_context),
        )
        if plan.startswith("Error"):
            return {"error": plan, "process_log": "\n".join(process_log)}
        process_log.append(f"> **è“å›¾æ‘˜è¦**ï¼š\n{plan[:200]}...\n")

        # 2) æ‰§è¡Œ
        process_log.append(f"**ğŸ’» ç¨‹åºå‘˜ ({_provider_label(roles['executor'])}) æ­£åœ¨ç¼–ç ...**")
        code_res = _call_llm(
            roles["executor"],
            api_keys[roles["executor"]],
            model_config,
            PROMPTS["executor"].format(plan=plan, data_context=enriched_data_context),
        )
        code = _extract_python_code(code_res)

        process_log.append("**âš™ï¸ è¿è¡Œä»£ç ...**")
        # æ”¯æŒæ–°çš„4å…ƒç»„è¿”å›ï¼š (output_text, image_path, plotly_json, new_df)
        result = execute_callback(code, df)
        if len(result) == 4:
            exec_text, exec_img, plotly_json, new_df = result
        else:
            # å‘åå…¼å®¹ï¼šå¦‚æœæ˜¯3å…ƒç»„ï¼Œæ·»åŠ  None ä½œä¸º plotly_json
            exec_text, exec_img, new_df = result
            plotly_json = None

        has_error = False
        if isinstance(exec_text, str) and (exec_text.startswith("Error") or "Traceback" in exec_text):
            has_error = True
            process_log.append(f"âš ï¸ **æŠ¥é”™**: `{exec_text[:120]}...`")

        # å·²çŸ¥æŠ¥é”™ï¼šç³»ç»Ÿè‡ªåŠ¨åé¦ˆï¼ˆæ›´å¿«ï¼Œä¸å¿…ç­‰è¯„å®¡å‘˜ï¼‰
        auto_fb = _auto_feedback_from_exec(exec_text)
        if has_error and auto_fb:
            process_log.append(f"ğŸ¤– **ç³»ç»Ÿè‡ªåŠ¨è¯Šæ–­**: {auto_fb}")
            feedback = auto_fb
            iteration += 1
            continue

        # 3) éªŒè¯
        process_log.append(f"**âš–ï¸ è¯„å®¡å‘˜ ({_provider_label(roles['verifier'])}) æ­£åœ¨å®¡æ ¸...**")
        force_fail = "\n\nâš ï¸ ä»£ç æŠ¥é”™ï¼Œè¯·åˆ¤ FAIL å¹¶è¯´æ˜åŸå› ä¸ä¿®å¤å»ºè®®ï¼" if has_error else ""
        verify_res = _call_llm(
            roles["verifier"],
            api_keys[roles["verifier"]],
            model_config,
            PROMPTS["verifier"].format(plan=plan, code=code, execution_result=exec_text) + force_fail,
        )

        review = _extract_json(verify_res)
        if not isinstance(review, dict):
            review = {}
        status = str(review.get("status") or "").upper()
        reason = str(review.get("reason") or "").strip() or "è¯„å®¡æœªç»™å‡ºæ˜ç¡®åŸå› "
        suggestion = str(review.get("suggestion") or "").strip()
        final_reply = str(review.get("final_reply") or "").strip()

        # è¯„å®¡è¾“å‡ºä¸å¯è§£æ/ç¼ºå­—æ®µæ—¶ï¼šä¸è¦é»˜è®¤ PASS
        if status not in ("PASS", "FAIL"):
            status = "FAIL" if has_error else "FAIL"
            if not suggestion:
                suggestion = "è¯„å®¡è¾“å‡ºæ— æ³•è§£æä¸ºåˆæ³• JSONã€‚è¯·ä¸¥æ ¼æŒ‰ JSON Schema è¾“å‡ºï¼Œå¹¶ä¿®å¤ä»£ç /ç»“æœä¸ºç©ºç­‰é—®é¢˜ã€‚"

        if status == "PASS":
            process_log.append(f"âœ… **éªŒè¯é€šè¿‡**: {reason}")
            return {
                "reply": final_reply
                if final_reply
                else f"### ğŸ¯ Radarm å¤šä¸“å®¶ç»“è®º\n\n**ç»“è®º**: {exec_text}\n\n**è¯„å®¡**: {reason}",
                "generated_code": code,
                "execution_result": exec_text,
                "image": exec_img,
                "plotly_json": plotly_json,  # æ–°å¢ï¼šPlotly å›¾è¡¨ JSON
                "new_df": new_df,
                "process_log": "\n".join(process_log),
            }

        process_log.append(f"âŒ **é©³å›**: {reason}\nğŸ”„ **å»ºè®®**: {suggestion}")
        feedback = suggestion
        iteration += 1

    return {
        "reply": f"âš ï¸ è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°ã€‚æœ€åç»“æœ: {exec_text}",
        "generated_code": code,
        "execution_result": exec_text,
        "image": exec_img,
        "plotly_json": plotly_json,  # æ–°å¢ï¼šPlotly å›¾è¡¨ JSON
        "new_df": new_df,
        "process_log": "\n".join(process_log),
    }


