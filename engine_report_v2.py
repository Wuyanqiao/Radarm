"""
Report Workflow v2 (multi-model, artifact-rich)
==============================================

ç›®æ ‡ï¼š
- å…è®¸åœ¨ä¸€é”®ç”ŸæˆæŠ¥å‘Šæ—¶ï¼Œçµæ´»ä½¿ç”¨ DeepSeek(chat/reasoner)ã€Zhipu(GLM-4.5/4.6/4.7)ã€Qwen(qwen-max/long/coder) ç­‰æ¨¡å‹å‚ä¸ä¸åŒé˜¶æ®µ
- ç”ŸæˆæŠ¥å‘Šçš„åŒæ—¶äº§å‡ºï¼šå›¾è¡¨ï¼ˆPNGï¼‰+ å›¾è¡¨æ•°æ®ï¼ˆCSVï¼‰+ manifestï¼ˆJSONï¼‰
- æŠ¥å‘Šäº§ç‰©ä¿å­˜åˆ° out/{session_id}/reports/{report_id}/ ä¸‹ï¼Œä¾¿äºå¯¼å‡ºä¸å¤šç‰ˆæœ¬ç®¡ç†

çº¦æŸï¼š
- ä¸åœ¨æ­¤å¤„åš FastAPI è·¯ç”±ï¼›ç”± backend.py è°ƒç”¨
"""

from __future__ import annotations

import json
import re
import time
import importlib
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import pandas as pd

from analysis_engine import run_analysis, TableOut, ChartOut, _safe_rel_path  # type: ignore


def _call_llm(
    provider: str,
    api_key: str,
    model_config: Dict[str, Any],
    prompt: str,
    *,
    model: Optional[str] = None,
    temperature: float = 0.2,
    timeout: int = 180,
) -> str:
    if not api_key:
        return f"Error: ç¼ºå°‘ {provider} Key"
    cfg = model_config.get(provider)
    if not cfg:
        return f"Error: æœªçŸ¥ provider={provider}"
    try:
        requests = importlib.import_module("requests")
    except Exception:
        return "Error: ç¼ºå°‘ requests ä¾èµ–"
    headers = {"Content-Type": "application/json", "Authorization": f"Bearer {api_key}"}
    payload = {
        "model": (model or cfg.get("model")),
        "messages": [{"role": "user", "content": prompt}],
        "temperature": float(temperature),
    }
    try:
        resp = requests.post(cfg.get("url"), headers=headers, json=payload, timeout=timeout)
        if resp.status_code != 200:
            return f"Error: {resp.status_code} {resp.text}"
        return resp.json()["choices"][0]["message"]["content"]
    except Exception as e:
        return f"Error: {str(e)}"


def _scan_first_json(text: str) -> Optional[Any]:
    if not text:
        return None
    s = str(text).strip()
    if not s:
        return None
    dec = json.JSONDecoder()
    for m in re.finditer(r"[\{\[]", s):
        try:
            obj, _end = dec.raw_decode(s[m.start() :])
            return obj
        except Exception:
            continue
    return None


def _extract_json_candidate(text: str) -> str:
    if not text:
        return ""
    s = str(text)
    m = re.search(r"```json(.*?)```", s, flags=re.IGNORECASE | re.DOTALL)
    if m:
        return m.group(1).strip()
    m = re.search(r"\{[\s\S]*\}", s)
    if m:
        return m.group(0).strip()
    m = re.search(r"\[[\s\S]*\]", s)
    if m:
        return m.group(0).strip()
    return s.strip()


def _safe_json_loads(text: str) -> Optional[Any]:
    if not text:
        return None
    s = str(text).strip()
    if not s:
        return None
    try:
        return json.loads(s)
    except Exception:
        pass
    obj = _scan_first_json(s)
    return obj


def _truncate(s: str, n: int) -> str:
    s2 = str(s or "")
    return s2 if len(s2) <= n else s2[:n] + "\n...(æˆªæ–­)"


def _stage_pick(stage_models: Dict[str, Any], stage: str, default_provider: str, default_model: str) -> Tuple[str, str]:
    cfg = (stage_models or {}).get(stage) if isinstance((stage_models or {}).get(stage), dict) else {}
    provider = str(cfg.get("provider") or default_provider)
    model = str(cfg.get("model") or default_model)
    return provider, model


def _ensure_report_dir(session_id: str, report_id: str) -> Tuple[Path, str]:
    sid = _safe_rel_path(session_id)
    rid = _safe_rel_path(report_id)
    rel = f"{sid}/reports/{rid}"
    p = Path("out") / sid / "reports" / rid
    p.mkdir(parents=True, exist_ok=True)
    (p / "data").mkdir(parents=True, exist_ok=True)
    return p, rel


def _save_tables_csv(report_dir: Path, tables: List[TableOut], *, job_key: str) -> List[Dict[str, Any]]:
    out = []
    for idx, t in enumerate(tables or []):
        name = getattr(t, "name", f"è¡¨æ ¼{idx+1}")
        md = getattr(t, "markdown", "") or ""
        df = getattr(t, "df", None)
        csv_rel = None
        if isinstance(df, pd.DataFrame) and len(df.columns) > 0:
            fname = f"data/{job_key}_table_{idx+1}.csv"
            try:
                df.to_csv(report_dir / fname, index=False, encoding="utf-8-sig")
                csv_rel = fname
            except Exception:
                csv_rel = None
        out.append({"name": name, "markdown": md, "csv": csv_rel})
    return out


def run_report_engine_v2(
    *,
    session_id: str,
    report_id: str,
    user_request: str,
    data_context: str,
    api_keys: Dict[str, str],
    model_config: Dict[str, Any],
    df: pd.DataFrame,
    stage_models: Dict[str, Any] = None,
    selected_columns: Optional[List[str]] = None,
    sample_rows: Optional[int] = None,
    check_cancelled: Optional[callable] = None,
) -> Dict[str, Any]:
    """
    è¿”å›ï¼š
    {
      report_id, title, content, log,
      artifacts: { base_dir, charts:[...], tables:[...], manifest_path, report_path },
      plan: {...}, insights: {...}
    }
    """
    stage_models = stage_models or {}
    t0 = time.time()
    log: List[str] = []

    report_dir, base_rel = _ensure_report_dir(session_id, report_id)

    # subset info (ä»…ç”¨äºå†™å…¥ manifest/æç¤ºï¼Œä¸åœ¨æ­¤å¤„ç­› dfï¼›ç­›é€‰ç”± backend å®Œæˆåä¼ å…¥ df)
    selected_columns = selected_columns or []
    sample_rows = int(sample_rows) if sample_rows else None

    # Stage A: plan
    planner_provider, planner_model = _stage_pick(stage_models, "planner", "deepseekA", "deepseek-reasoner")
    log.append(f"### ğŸ§­ Stage Aï¼šè§„åˆ’ï¼ˆ{planner_provider} / {planner_model}ï¼‰")
    allowed = [
        "overview",
        "descriptive",
        "frequency",
        "crosstab",
        "group_summary",
        "normality",
        "correlation",
        "linear_regression",
        "logistic_regression",
        "pca",
        "kmeans",
        "ttest",
        "anova",
        "chi_square",
        "nonparam",
    ]
    plan_prompt = (
        "ä½ æ˜¯ Radarm çš„ã€æŠ¥å‘Šè§„åˆ’å™¨ã€‘ã€‚è¯·åŸºäºç”¨æˆ·éœ€æ±‚ä¸æ•°æ®æ¦‚å†µï¼Œè§„åˆ’ä¸€ä»½å¯äº¤ä»˜çš„æ•°æ®åˆ†ææŠ¥å‘Šã€‚\n"
        "ä½ å¿…é¡»è¾“å‡º 1 ä¸ªä¸¥æ ¼ JSON å¯¹è±¡ï¼ˆä¸è¦ Markdown/ä¸è¦ä»£ç å—/ä¸è¦å‰åç¼€ï¼‰ã€‚\n"
        "JSON Schemaï¼š\n"
        "{\n"
        '  "title": "æŠ¥å‘Šæ ‡é¢˜",\n'
        '  "jobs": [ {"analysis": "...", "params": {...}}, ... ],\n'
        '  "sections": [ {"title": "...", "job_indexes": [0,1,...], "notes": "..."}, ... ],\n'
        '  "assumptions": ["..."],\n'
        '  "risks": ["..."]\n'
        "}\n"
        "çº¦æŸï¼š\n"
        f"- analysis åªèƒ½ä»ä»¥ä¸‹åˆ—è¡¨é€‰æ‹©ï¼š{allowed}\n"
        "- jobs æœ€å¤š 10 ä¸ª\n"
        "- params ä¸­æ¶‰åŠåˆ—åå¿…é¡»ä½¿ç”¨çœŸå®åˆ—åï¼ˆæ¥è‡ªæ•°æ®æ¦‚å†µï¼‰\n\n"
        f"[ç”¨æˆ·éœ€æ±‚]\n{user_request}\n\n"
        f"[æ•°æ®æ¦‚å†µ]\n{_truncate(data_context, 12000)}\n"
    )
    plan_text = _call_llm(planner_provider, api_keys.get(planner_provider, ""), model_config, plan_prompt, model=planner_model, temperature=0.2, timeout=180)
    if plan_text.startswith("Error"):
        return {"error": plan_text, "log": "\n".join(log)}
    plan_obj = _safe_json_loads(_extract_json_candidate(plan_text))
    if not isinstance(plan_obj, dict):
        # fallback plan
        plan_obj = {
            "title": "æ•°æ®åˆ†ææŠ¥å‘Š",
            "jobs": [{"analysis": "overview", "params": {}}, {"analysis": "descriptive", "params": {"columns": []}}],
            "sections": [{"title": "æ•°æ®æ¦‚è§ˆä¸æè¿°ç»Ÿè®¡", "job_indexes": [0, 1], "notes": "è‡ªåŠ¨å…œåº•ï¼šæ¦‚è§ˆ+æè¿°ç»Ÿè®¡"}],
            "assumptions": [],
            "risks": ["è§„åˆ’è¾“å‡ºæ— æ³•è§£æï¼Œå·²å¯ç”¨å…œåº•æ–¹æ¡ˆ"],
        }
    # sanitize jobs
    jobs_in = plan_obj.get("jobs") or []
    jobs: List[Dict[str, Any]] = []
    for j in jobs_in[:10]:
        if not isinstance(j, dict):
            continue
        a = str(j.get("analysis") or "").strip()
        if a not in allowed:
            continue
        p = j.get("params") or {}
        jobs.append({"analysis": a, "params": p if isinstance(p, dict) else {}})
    if not jobs:
        jobs = [{"analysis": "overview", "params": {}}, {"analysis": "descriptive", "params": {"columns": []}}]
    plan_obj["jobs"] = jobs

    # æ£€æŸ¥æ˜¯å¦å·²å–æ¶ˆ
    if check_cancelled and check_cancelled():
        log.append("\nâš ï¸ æŠ¥å‘Šç”Ÿæˆå·²å–æ¶ˆï¼ˆè§„åˆ’é˜¶æ®µåï¼‰")
        return {"cancelled": True, "log": "\n".join(log), "error": "æŠ¥å‘Šç”Ÿæˆå·²å–æ¶ˆ"}

    # Stage B: execute deterministic analyses (charts+tables)
    log.append("\n### ğŸ§® Stage Bï¼šè®¡ç®—ä¸äº§ç‰©ï¼ˆç¡®å®šæ€§å¼•æ“ï¼‰")
    artifacts_tables: List[Dict[str, Any]] = []
    artifacts_charts: List[Dict[str, Any]] = []
    job_results: List[Dict[str, Any]] = []

    out_subdir = f"reports/{_safe_rel_path(report_id)}"
    for idx, job in enumerate(jobs):
        # æ£€æŸ¥æ˜¯å¦å·²å–æ¶ˆ
        if check_cancelled and check_cancelled():
            log.append(f"\nâš ï¸ æŠ¥å‘Šç”Ÿæˆå·²å–æ¶ˆï¼ˆæ‰§è¡Œåˆ°ç¬¬ {idx+1}/{len(jobs)} ä¸ªåˆ†æï¼‰")
            return {"cancelled": True, "log": "\n".join(log), "error": "æŠ¥å‘Šç”Ÿæˆå·²å–æ¶ˆ"}
        analysis = job.get("analysis")
        params = job.get("params") or {}
        job_key = f"job{idx+1}_{analysis}"
        log.append(f"- è¿è¡Œï¼š{analysis}  params={_truncate(json.dumps(params, ensure_ascii=False), 400)}")
        try:
            res = run_analysis(session_id=session_id, df=df, analysis=str(analysis), params=params, out_subdir=out_subdir)
        except Exception as e:
            log.append(f"  âš ï¸ å¤±è´¥ï¼š{str(e)}")
            continue

        tables: List[TableOut] = res.get("tables") or []
        charts: List[ChartOut] = res.get("charts") or []
        summary = res.get("summary") or {}

        t_items = _save_tables_csv(report_dir, tables, job_key=job_key)
        for t in t_items:
            # æŠŠ csv ç›¸å¯¹è·¯å¾„è¡¥å…¨ä¸º /out ç›¸å¯¹è·¯å¾„
            csv_rel = t.get("csv")
            if csv_rel:
                t["csv_path"] = f"{base_rel}/{csv_rel}"
            else:
                t["csv_path"] = None
            artifacts_tables.append({"job": job_key, **t})

        for c in charts or []:
            name = getattr(c, "name", "å›¾")
            path = getattr(c, "path", None)
            if path:
                artifacts_charts.append({"job": job_key, "name": name, "path": path})

        job_results.append(
            {
                "job": job_key,
                "analysis": analysis,
                "params": params,
                "tables": [{"name": t.get("name"), "csv_path": t.get("csv_path"), "markdown": _truncate(t.get("markdown") or "", 3000)} for t in t_items],
                "charts": [{"name": x.get("name"), "path": x.get("path")} for x in artifacts_charts if x.get("job") == job_key],
                "summary": summary,
            }
        )

    # æ£€æŸ¥æ˜¯å¦å·²å–æ¶ˆ
    if check_cancelled and check_cancelled():
        log.append("\nâš ï¸ æŠ¥å‘Šç”Ÿæˆå·²å–æ¶ˆï¼ˆè®¡ç®—é˜¶æ®µåï¼‰")
        return {"cancelled": True, "log": "\n".join(log), "error": "æŠ¥å‘Šç”Ÿæˆå·²å–æ¶ˆ"}

    # Stage C: insights
    analyst_provider, analyst_model = _stage_pick(stage_models, "analyst", "deepseekB", "deepseek-reasoner")
    log.append(f"\n### ğŸ’¡ Stage Cï¼šæ´å¯Ÿï¼ˆ{analyst_provider} / {analyst_model}ï¼‰")
    insight_prompt = (
        "ä½ æ˜¯ä¸€åä¸¥è°¨çš„ã€ä¸šåŠ¡æ´å¯Ÿåˆ†æå¸ˆã€‘ã€‚è¯·åŸºäºä¸‹è¿°åˆ†æç»“æœï¼Œæç‚¼æ´å¯Ÿï¼ˆä¸è¦æœæ’°æ•°å­—ï¼‰ã€‚\n"
        "åªè¾“å‡º 1 ä¸ªä¸¥æ ¼ JSON å¯¹è±¡ï¼š\n"
        "{\n"
        '  "findings": [ {"id":"F1","title":"...","evidence":"å¼•ç”¨è¡¨æ ¼/ç»Ÿè®¡é‡","confidence":"high|mid|low","tags":["..."]}, ... ],\n'
        '  "next_steps": ["..."],\n'
        '  "data_issues": ["..."]\n'
        "}\n\n"
        f"[ç”¨æˆ·éœ€æ±‚]\n{user_request}\n\n"
        f"[åˆ†æç»“æœ(æˆªæ–­)]\n{_truncate(json.dumps(job_results, ensure_ascii=False, indent=2), 14000)}\n"
    )
    insight_text = _call_llm(analyst_provider, api_keys.get(analyst_provider, ""), model_config, insight_prompt, model=analyst_model, temperature=0.3, timeout=180)
    insights_obj = _safe_json_loads(_extract_json_candidate(insight_text))
    if not isinstance(insights_obj, dict):
        insights_obj = {"findings": [], "next_steps": [], "data_issues": ["æ´å¯Ÿé˜¶æ®µè¾“å‡ºæ— æ³•è§£æï¼Œå·²è·³è¿‡ã€‚"]}

    # æ£€æŸ¥æ˜¯å¦å·²å–æ¶ˆ
    if check_cancelled and check_cancelled():
        log.append("\nâš ï¸ æŠ¥å‘Šç”Ÿæˆå·²å–æ¶ˆï¼ˆæ´å¯Ÿé˜¶æ®µåï¼‰")
        return {"cancelled": True, "log": "\n".join(log), "error": "æŠ¥å‘Šç”Ÿæˆå·²å–æ¶ˆ"}

    # Stage D: write markdown
    writer_provider, writer_model = _stage_pick(stage_models, "writer", "zhipu", "glm-4.7")
    log.append(f"\n### ğŸ“ Stage Dï¼šæˆæ–‡ï¼ˆ{writer_provider} / {writer_model}ï¼‰")

    charts_index = "\n".join([f"- {c['name']}: /out/{c['path']}" for c in artifacts_charts[:30]])
    tables_index = "\n".join([f"- {t['name']}: {t.get('csv_path') or 'ï¼ˆæ— CSVï¼‰'}" for t in artifacts_tables[:30]])

    writer_prompt = (
        "ä½ æ˜¯ä¸€åã€æ•°æ®åˆ†ææŠ¥å‘Šä¸»ç¬”ã€‘ã€‚è¯·æŠŠææ–™å†™æˆä¸€ä»½å¯äº¤ä»˜æŠ¥å‘Šï¼ˆMarkdownï¼‰ã€‚\n"
        "å¼ºçº¦æŸï¼š\n"
        "- ä¸è¦æœæ’°ä»»ä½•æ•°å­—ï¼›æ¶‰åŠç»Ÿè®¡é‡/æ•°å€¼å¿…é¡»æ¥è‡ªç»™å®šçš„ job_results/summary æˆ–è¡¨æ ¼ã€‚\n"
        "- å…è®¸å†™æ¸…æ¥šâ€œæ•°æ®ä¸è¶³/å­—æ®µç¼ºå¤±â€ã€‚\n"
        "- æŠ¥å‘Šéœ€åŒ…å«ï¼šæ‰§è¡Œæ‘˜è¦ã€æ•°æ®æ¦‚å†µã€æ ¸å¿ƒå‘ç°ï¼ˆåˆ†ç‚¹+è¯æ®ï¼‰ã€å›¾è¡¨è§£è¯»ã€å»ºè®®ä¸é£é™©ã€é™„å½•ã€‚\n"
        "- å›¾è¡¨å¯ç”¨ Markdown å›¾ç‰‡è¯­æ³•å¼•ç”¨ï¼š![](/out/<path>)ã€‚\n\n"
        f"[æŠ¥å‘Šæ ‡é¢˜(å»ºè®®)]\n{plan_obj.get('title','æ•°æ®åˆ†ææŠ¥å‘Š')}\n\n"
        f"[ç”¨æˆ·éœ€æ±‚]\n{user_request}\n\n"
        f"[æŠ¥å‘Šç»“æ„å»ºè®®]\n{_truncate(json.dumps(plan_obj.get('sections') or [], ensure_ascii=False, indent=2), 4000)}\n\n"
        f"[æ´å¯Ÿ(JSON)]\n{_truncate(json.dumps(insights_obj, ensure_ascii=False, indent=2), 9000)}\n\n"
        f"[åˆ†æç»“æœ(æˆªæ–­ JSON)]\n{_truncate(json.dumps(job_results, ensure_ascii=False, indent=2), 14000)}\n\n"
        f"[å›¾è¡¨ç´¢å¼•]\n{_truncate(charts_index, 3000)}\n\n"
        f"[è¡¨æ ¼æ•°æ®ç´¢å¼•(CSV)]\n{_truncate(tables_index, 3000)}\n\n"
        "è¯·åªè¾“å‡º Markdown æ­£æ–‡ã€‚\n"
    )
    draft_md = _call_llm(writer_provider, api_keys.get(writer_provider, ""), model_config, writer_prompt, model=writer_model, temperature=0.2, timeout=240)
    if draft_md.startswith("Error"):
        return {"error": draft_md, "log": "\n".join(log)}

    # æ£€æŸ¥æ˜¯å¦å·²å–æ¶ˆ
    if check_cancelled and check_cancelled():
        log.append("\nâš ï¸ æŠ¥å‘Šç”Ÿæˆå·²å–æ¶ˆï¼ˆæˆæ–‡é˜¶æ®µåï¼‰")
        return {"cancelled": True, "log": "\n".join(log), "error": "æŠ¥å‘Šç”Ÿæˆå·²å–æ¶ˆ"}

    # Stage E: reviewer (optional)
    reviewer_provider, reviewer_model = _stage_pick(stage_models, "reviewer", "deepseekC", "deepseek-reasoner")
    log.append(f"\n### ğŸ§ª Stage Eï¼šå®¡æ ¡ï¼ˆ{reviewer_provider} / {reviewer_model}ï¼‰")
    review_prompt = (
        "ä½ æ˜¯ä¸€åã€ä¸¥æ ¼å®¡æ ¡å‘˜ã€‘ã€‚ä»»åŠ¡ï¼šæ£€æŸ¥æŠ¥å‘Šæ˜¯å¦å­˜åœ¨æœæ’°æ•°å­—ã€è¯æ®ä¸è¶³ã€é€»è¾‘è·³è·ƒã€‚\n"
        "åªè¾“å‡º 1 ä¸ªä¸¥æ ¼ JSON å¯¹è±¡ï¼š\n"
        "{\n"
        '  "status": "PASS"|"FAIL",\n'
        '  "issues": ["..."],\n'
        '  "fixed_markdown": "å¦‚æœ FAILï¼Œç»™å‡ºä¿®è®¢åçš„å®Œæ•´ Markdownï¼›å¦‚æœ PASS å¯ç•™ç©º"\n'
        "}\n\n"
        f"[åˆ†æç»“æœ(æˆªæ–­)]\n{_truncate(json.dumps(job_results, ensure_ascii=False, indent=2), 12000)}\n\n"
        f"[æŠ¥å‘Šè‰ç¨¿]\n{_truncate(draft_md, 12000)}\n"
    )
    review_text = _call_llm(reviewer_provider, api_keys.get(reviewer_provider, ""), model_config, review_prompt, model=reviewer_model, temperature=0.1, timeout=200)
    review_obj = _safe_json_loads(_extract_json_candidate(review_text))
    final_md = draft_md
    if isinstance(review_obj, dict) and str(review_obj.get("status")).upper() == "FAIL":
        fixed = str(review_obj.get("fixed_markdown") or "").strip()
        if fixed:
            final_md = fixed
        issues = review_obj.get("issues") or []
        try:
            log.append("å®¡æ ¡ï¼šFAIL")
            log.append(_truncate(json.dumps(issues, ensure_ascii=False, indent=2), 2000))
        except Exception:
            pass
    else:
        log.append("å®¡æ ¡ï¼šPASS")

    title = str(plan_obj.get("title") or "æ•°æ®åˆ†ææŠ¥å‘Š").strip() or "æ•°æ®åˆ†ææŠ¥å‘Š"

    # save files
    log_text = "\n".join(log)
    (report_dir / "report.md").write_text(final_md, encoding="utf-8")
    manifest = {
        "report_id": report_id,
        "title": title,
        "created_at": float(time.time()),
        "selected_columns": selected_columns,
        "sample_rows": sample_rows,
        "stage_models": stage_models,
        "plan": plan_obj,
        "jobs": job_results,
        "insights": insights_obj,
        "artifacts": {"base_dir": base_rel, "charts": artifacts_charts, "tables": artifacts_tables},
        "process_log": log_text,
    }
    (report_dir / "manifest.json").write_text(json.dumps(manifest, ensure_ascii=False, indent=2), encoding="utf-8")
    (report_dir / "process_log.md").write_text(log_text, encoding="utf-8")

    elapsed_ms = int((time.time() - t0) * 1000)
    log.append(f"\nâœ… å®Œæˆï¼Œç”¨æ—¶ {elapsed_ms}ms")

    return {
        "report_id": report_id,
        "title": title,
        "content": final_md,
        "log": "\n".join(log),
        "artifacts": {
            "base_dir": base_rel,
            "report_path": f"{base_rel}/report.md",
            "manifest_path": f"{base_rel}/manifest.json",
            "process_log_path": f"{base_rel}/process_log.md",
            "charts": artifacts_charts,
            "tables": artifacts_tables,
        },
        "plan": plan_obj,
        "insights": insights_obj,
        "elapsed_ms": elapsed_ms,
    }


