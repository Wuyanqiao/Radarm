"""
æ•°æ®åˆ†ææŠ¥å‘Šç”Ÿæˆå¼•æ“ï¼ˆäº”é˜¶æ®µï¼Œå¤šä¸“å®¶æ··åˆ-æŠ¥å‘Šç‰ˆï¼‰
================================================

æœ¬å¼•æ“åªæœåŠ¡â€œç”Ÿæˆæ•°æ®åˆ†ææŠ¥å‘Šâ€çš„ç‹¬ç«‹å·¥ä½œæµï¼ˆ/reportï¼‰ã€‚

äº”é˜¶æ®µï¼ˆè§’è‰²å›ºå®šï¼Œé»˜è®¤ä½¿ç”¨ DeepSeek-A/B/C ä¸‰å¥—æ§½ä½ï¼‰ï¼š
1) æ•°æ®é¢„å¤„ç†ä¸ä»»åŠ¡æ‹†è§£ï¼ˆé¡¹ç›®ç»ç†ï¼šDeepSeek-Cï¼‰
2) ç¡¬æ ¸é€»è¾‘åˆ†æä¸è®¡ç®—ï¼ˆé¦–å¸­ç§‘å­¦å®¶ï¼šDeepSeek-Aï¼‰â€”â€” ç”Ÿæˆå¹¶æ‰§è¡Œ Python ä»£ç ï¼Œäº§å‡ºâ€œä¸­é—´æ€æ•°æ®åŒ…â€(JSON)
3) ä¸šåŠ¡æ´å¯Ÿä¸æ¨ªå‘å…³è”ï¼ˆä¸šåŠ¡é¡¾é—®ï¼šDeepSeek-Bï¼‰â€”â€” äº§å‡ºâ€œæ´å¯Ÿå»ºè®®åˆ—è¡¨â€(JSON)
4) å†²çªè§£å†³ä¸æ·±åº¦ç»¼è¿°ï¼ˆé¦–å¸­ç§‘å­¦å®¶å›å½’ï¼šDeepSeek-Aï¼‰â€”â€” äº§å‡ºâ€œæŠ€æœ¯æ€§æ‘˜è¦â€(JSON)
5) æœ€ç»ˆæŠ¥å‘Šç”Ÿæˆä¸æ’ç‰ˆï¼ˆä¸»ç¬”ï¼šDeepSeek-Cï¼‰â€”â€” äº§å‡º Markdown æŠ¥å‘Š

è¾“å‡ºå¥‘çº¦ï¼ˆå¼ºçº¦æŸï¼‰ï¼š
- Stage 1/3/4ï¼šåªè¾“å‡º 1 ä¸ª JSON å¯¹è±¡ï¼Œå¿…é¡»èƒ½è¢« json.loads è§£æï¼ˆä¸è¦ Markdown/è§£é‡Šæ–‡å­—ï¼‰
- Stage 2ï¼šåªè¾“å‡º 1 ä¸ª ```python ä»£ç å—ï¼›ä»£ç è¿è¡Œåå¿…é¡» print 1 ä¸ª JSONï¼ˆä¸­é—´æ€æ•°æ®åŒ…ï¼‰ï¼Œå°½é‡ä¸è¦é¢å¤– print
- Stage 5ï¼šåªè¾“å‡º Markdown æ­£æ–‡
"""

import json
import re
import time
import importlib
import ast
from typing import Any, Dict, List, Optional


def _call_llm(
    provider: str,
    api_key: str,
    model_config: Dict[str, Any],
    prompt: str,
    *,
    temperature: float = 0.2,
    timeout: int = 120,
    retries: int = 2,
) -> str:
    if not api_key:
        return f"Error: ç¼ºå°‘ {provider} Key"
    cfg = model_config.get(provider)
    if not cfg:
        return f"Error: æœªçŸ¥æ¨¡å‹ provider={provider}"

    try:
        requests = importlib.import_module("requests")
    except Exception:
        return "Error: ç¼ºå°‘ requests ä¾èµ–"

    headers = {"Content-Type": "application/json", "Authorization": f"Bearer {api_key}"}
    payload = {"model": cfg["model"], "messages": [{"role": "user", "content": prompt}], "temperature": temperature}

    for _ in range(max(1, retries)):
        try:
            resp = requests.post(cfg["url"], headers=headers, json=payload, timeout=timeout)
            if resp.status_code == 200:
                return resp.json()["choices"][0]["message"]["content"]
        except Exception:
            time.sleep(1)
    return "Error: API è°ƒç”¨å¤±è´¥"


def _extract_python_code(text: str) -> str:
    m = re.search(r"```python(.*?)```", text, re.DOTALL | re.IGNORECASE)
    return m.group(1).strip() if m else text.strip()


def _extract_json_candidate(text: str) -> Optional[str]:
    """
    ä»æ¨¡å‹è¾“å‡º/ä»£ç æ‰§è¡Œè¾“å‡ºä¸­å°½é‡æå– JSONï¼ˆå­—ç¬¦ä¸²ï¼‰ã€‚
    """
    if not text:
        return None

    # 1) ```json ... ```
    m = re.search(r"```json(.*?)```", text, re.DOTALL | re.IGNORECASE)
    if m:
        return m.group(1).strip()

    # 2) å¤§æ‹¬å·å¯¹è±¡
    m = re.search(r"\{[\s\S]*\}", text)
    if m:
        return m.group(0).strip()

    # 3) æ–¹æ‹¬å·æ•°ç»„
    m = re.search(r"\[[\s\S]*\]", text)
    if m:
        return m.group(0).strip()

    return None


def _safe_json_loads(text: str) -> Optional[Any]:
    if text is None:
        return None
    s = str(text).strip()
    if not s:
        return None

    # 1) strict json
    try:
        return json.loads(s)
    except Exception:
        pass

    # 2) scan first valid json object/array inside a larger text
    obj = _scan_first_json(s)
    if obj is not None:
        return obj

    # 3) normalize common JSON-ish issues then retry
    s2 = _normalize_jsonish(s)
    try:
        return json.loads(s2)
    except Exception:
        pass

    obj = _scan_first_json(s2)
    if obj is not None:
        return obj

    # 4) python-literal fallback (handles single quotes/None/True/False)
    py = _to_python_literal(s2)
    try:
        return ast.literal_eval(py)
    except Exception:
        return None


def _json_dumps(obj: Any, *, max_chars: int = 30000) -> str:
    """
    å°†å¯¹è±¡åºåˆ—åŒ–ä¸º JSON å­—ç¬¦ä¸²ï¼Œå¹¶å¯¹é•¿åº¦åšä¸Šé™ä¿æŠ¤ï¼ˆé¿å…æ’‘çˆ† promptï¼‰ã€‚
    """
    s = json.dumps(obj, ensure_ascii=False, indent=2)
    if len(s) <= max_chars:
        return s
    return s[:max_chars] + "\n...(æˆªæ–­)"


def _call_llm_expect_json_dict(
    provider: str,
    api_key: str,
    model_config: Dict[str, Any],
    prompt: str,
    *,
    stage_name: str,
    temperature: float,
    timeout: int,
    retries: int = 2,
) -> Dict[str, Any]:
    """
    è°ƒç”¨æ¨¡å‹å¹¶å¼ºåˆ¶æ‹¿åˆ°å¯è§£æçš„ JSON å¯¹è±¡ï¼ˆdictï¼‰ã€‚
    å¦‚æœå¤šæ¬¡ä»å¤±è´¥ï¼Œè¿”å› {"error": "..."}ã€‚
    """
    last_text = ""
    for attempt in range(max(1, retries)):
        last_text = _call_llm(
            provider,
            api_key,
            model_config,
            prompt,
            temperature=temperature,
            timeout=timeout,
            retries=1,
        )
        if last_text.startswith("Error"):
            return {"error": last_text}

        cand = _extract_json_candidate(last_text) or last_text.strip()
        obj = _safe_json_loads(cand)
        if obj is None:
            # å†å°è¯•ç›´æ¥åœ¨å®Œæ•´æ–‡æœ¬ä¸­æ‰«æ JSONï¼ˆé˜²æ­¢ regex æŠ“åˆ°å¤šæ®µ {} å¯¼è‡´å¤±è´¥ï¼‰
            obj = _safe_json_loads(last_text)
        if isinstance(obj, dict):
            return {"obj": obj, "text": _json_dumps(obj)}

        # è¿½åŠ ä¸€æ¬¡çº é”™æç¤ºé‡è¯•
        prompt = (
            prompt
            + "\n\n[æ ¼å¼çº é”™] ä½ ä¸Šæ¬¡è¾“å‡ºæ— æ³•è¢« json.loads è§£æã€‚è¯·ä¸¥æ ¼åªè¾“å‡ºä¸€ä¸ª JSON å¯¹è±¡ï¼š"
            + "ä¸è¦ Markdownï¼Œä¸è¦ä»£ç å—ï¼Œä¸è¦é¢å¤–è§£é‡Šæ–‡å­—ï¼Œä¸è¦å¤šä½™å‰åç¼€ã€‚"
        )

    return {"error": f"{stage_name} è¾“å‡ºæ— æ³•è§£æä¸º JSON å¯¹è±¡", "raw": last_text[:2000]}


def _scan_first_json(text: str) -> Optional[Any]:
    """
    åœ¨åŒ…å«å™ªå£°çš„æ–‡æœ¬ä¸­æ‰«æç¬¬ä¸€ä¸ªå¯è§£æçš„ JSON å¯¹è±¡/æ•°ç»„ã€‚
    """
    if not text:
        return None
    decoder = json.JSONDecoder()
    for m in re.finditer(r"[\{\[]", text):
        start = m.start()
        try:
            obj, _end = decoder.raw_decode(text[start:])
            return obj
        except Exception:
            continue
    return None


def _normalize_jsonish(s: str) -> str:
    """
    å°†å¸¸è§çš„â€œJSON-ishâ€è¾“å‡ºå°½é‡è§„èŒƒåŒ–ä¸ºä¸¥æ ¼ JSONï¼ˆä»ä¸ä¿è¯ 100%ï¼‰ã€‚
    """
    t = s.strip()
    # ç»Ÿä¸€å¼•å·ï¼ˆä¸­æ–‡å¼•å·/èŠ±å¼•å·ï¼‰
    t = t.replace("â€œ", '"').replace("â€", '"').replace("â€˜", "'").replace("â€™", "'")
    # å»æ³¨é‡Š
    t = re.sub(r"//.*?$", "", t, flags=re.MULTILINE)
    t = re.sub(r"/\*[\s\S]*?\*/", "", t)
    # å»å°¾é€—å·
    t = re.sub(r",(\s*[}\]])", r"\1", t)
    # NaN/Infinity å…œåº•
    t = re.sub(r"\bNaN\b", "null", t)
    t = re.sub(r"\bInfinity\b", "null", t)
    t = re.sub(r"\b-Infinity\b", "null", t)
    return t


def _to_python_literal(s: str) -> str:
    """
    æŠŠ JSON-ish å­—ç¬¦ä¸²å°½é‡è½¬æˆ Python å­—é¢é‡ï¼Œä¾› ast.literal_eval å°è¯•è§£æã€‚
    """
    t = s.strip()
    t = t.replace("null", "None").replace("true", "True").replace("false", "False")
    return t

def _ensure_list(v: Any) -> List[Any]:
    if v is None:
        return []
    if isinstance(v, list):
        return v
    return [v]


DOMAIN_TEMPLATES: Dict[str, Dict[str, Any]] = {
    "generic": {
        "kpi_definitions": [
            {
                "kpi": "è®°å½•æ•°",
                "description": "æ•°æ®è¡¨æ€»è®°å½•æ•°",
                "formula": "n_rows = len(df)",
                "required_columns": [],
                "group_by": [],
                "time_grain": "none",
                "directionality": "unknown",
            },
            {
                "kpi": "ç¼ºå¤±ç‡Top",
                "description": "ç¼ºå¤±å€¼å æ¯”æœ€é«˜çš„å­—æ®µåˆ—è¡¨",
                "formula": "missing_pct = df.isna().mean()",
                "required_columns": [],
                "group_by": [],
                "time_grain": "none",
                "directionality": "lower_is_better",
            },
            {
                "kpi": "é‡å¤è¡Œæ•°",
                "description": "å®Œå…¨é‡å¤çš„è®°å½•æ•°é‡",
                "formula": "dup = df.duplicated().sum()",
                "required_columns": [],
                "group_by": [],
                "time_grain": "none",
                "directionality": "lower_is_better",
            },
        ],
        "trend_questions": [
            {"question": "æ ¸å¿ƒæ•°å€¼æŒ‡æ ‡æ˜¯å¦å­˜åœ¨æ˜æ˜¾è¶‹åŠ¿æˆ–ç»“æ„æ€§å˜åŒ–ï¼Ÿ", "method": "æ—¶é—´åºåˆ—/åˆ†ç»„å¯¹æ¯”/ç›¸å…³æ€§", "required_columns": []}
        ],
        "anomaly_scan_plan": [
            {
                "name": "æ•°å€¼åˆ—æç«¯å€¼æ‰«æ",
                "method": "IQR",
                "target_columns": [],
                "threshold_or_rule": "å¯¹æ¯ä¸ªæ•°å€¼åˆ—ä½¿ç”¨ IQR(1.5) æ‰¾ TopN æç«¯å€¼",
                "flag": "é‡ç‚¹å…³æ³¨åŒºåŸŸ",
            },
            {
                "name": "é€»è¾‘ä¸ä¸€è‡´æ‰«æ",
                "method": "rule_based",
                "target_columns": [],
                "threshold_or_rule": "æ£€æŸ¥è´Ÿå€¼/æ¯”ä¾‹>1/é‡‘é¢ä¸º0ä½†æ•°é‡>0ç­‰å¸¸è§é€»è¾‘æ–­å±‚",
                "flag": "å¾…äººå·¥å¤æ ¸",
            },
        ],
    },
    "sales": {
        "kpi_definitions": [
            {
                "kpi": "GMV/é”€å”®é¢",
                "description": "æˆäº¤é‡‘é¢æˆ–é”€å”®æ€»é¢",
                "formula": "sum(amount)",
                "required_columns": ["amount_or_revenue"],
                "group_by": [],
                "time_grain": "day",
                "directionality": "higher_is_better",
            },
            {
                "kpi": "è®¢å•æ•°",
                "description": "è®¢å•é‡ï¼ˆå»é‡è®¢å•IDï¼‰",
                "formula": "nunique(order_id)",
                "required_columns": ["order_id"],
                "group_by": [],
                "time_grain": "day",
                "directionality": "higher_is_better",
            },
            {
                "kpi": "å®¢å•ä»·(AOV)",
                "description": "å¹³å‡æ¯å•é‡‘é¢",
                "formula": "GMV / è®¢å•æ•°",
                "required_columns": ["amount_or_revenue", "order_id"],
                "group_by": [],
                "time_grain": "day",
                "directionality": "higher_is_better",
            },
            {
                "kpi": "ç”¨æˆ·æ•°",
                "description": "ä¸‹å•/è®¿é—®ç”¨æˆ·æ•°ï¼ˆå»é‡ user_idï¼‰",
                "formula": "nunique(user_id)",
                "required_columns": ["user_id"],
                "group_by": [],
                "time_grain": "day",
                "directionality": "higher_is_better",
            },
            {
                "kpi": "äººå‡æ¶ˆè´¹(ARPU)",
                "description": "GMV/ç”¨æˆ·æ•°",
                "formula": "GMV / ç”¨æˆ·æ•°",
                "required_columns": ["amount_or_revenue", "user_id"],
                "group_by": [],
                "time_grain": "day",
                "directionality": "higher_is_better",
            },
        ],
        "trend_questions": [
            {"question": "GMV/é”€å”®é¢æ˜¯å¦å­˜åœ¨å­£èŠ‚æ€§/æ´»åŠ¨å³°å€¼ï¼ˆå¦‚åŒ11ï¼‰ï¼Ÿ", "method": "æŒ‰å¤©/å‘¨/æœˆè¶‹åŠ¿ + å³°å€¼æ£€æµ‹", "required_columns": ["time", "amount_or_revenue"]},
            {"question": "ä¸åŒæ¸ é“/åœ°åŒº/å“ç±»å¯¹ GMV çš„è´¡çŒ®ä¸è¶‹åŠ¿å¦‚ä½•ï¼Ÿ", "method": "åˆ†ç»„æ±‡æ€» + TopN å¯¹æ¯”", "required_columns": ["amount_or_revenue"]},
            {"question": "å®¢å•ä»·æ˜¯å¦å‘ç”Ÿç»“æ„æ€§å˜åŒ–ï¼ˆä¸Šæ¶¨/ä¸‹æ»‘ï¼‰ï¼Ÿ", "method": "AOV æ—¶é—´åºåˆ— + åˆ†ç»„å¯¹æ¯”", "required_columns": ["order_id", "amount_or_revenue"]},
        ],
        "anomaly_scan_plan": [
            {
                "name": "è®¢å•é‡‘é¢å¼‚å¸¸",
                "method": "zscore",
                "target_columns": ["amount_or_revenue"],
                "threshold_or_rule": "|z| > 3 çš„å•ç¬”é‡‘é¢/èšåˆé‡‘é¢",
                "flag": "é‡ç‚¹å…³æ³¨åŒºåŸŸ",
            },
            {
                "name": "è´Ÿæ•°/ä¸åˆç†å€¼æ‰«æï¼ˆé”€å”®ï¼‰",
                "method": "rule_based",
                "target_columns": ["amount_or_revenue", "quantity"],
                "threshold_or_rule": "é‡‘é¢<0ã€æ•°é‡<0ã€é‡‘é¢==0ä½†æ•°é‡>0 ç­‰",
                "flag": "å¾…äººå·¥å¤æ ¸",
            },
        ],
    },
    "finance": {
        "kpi_definitions": [
            {
                "kpi": "æ”¶å…¥/å…¥è´¦é‡‘é¢",
                "description": "æ”¶å…¥æˆ–å…¥è´¦æ€»é¢",
                "formula": "sum(revenue)",
                "required_columns": ["revenue"],
                "group_by": [],
                "time_grain": "month",
                "directionality": "higher_is_better",
            },
            {
                "kpi": "æˆæœ¬/è´¹ç”¨",
                "description": "æˆæœ¬æˆ–è´¹ç”¨æ€»é¢",
                "formula": "sum(cost)",
                "required_columns": ["cost"],
                "group_by": [],
                "time_grain": "month",
                "directionality": "lower_is_better",
            },
            {
                "kpi": "åˆ©æ¶¦",
                "description": "åˆ©æ¶¦=æ”¶å…¥-æˆæœ¬ï¼ˆè‹¥å­˜åœ¨åˆ©æ¶¦åˆ—åˆ™ç›´æ¥ç”¨ï¼‰",
                "formula": "revenue - cost",
                "required_columns": ["revenue", "cost"],
                "group_by": [],
                "time_grain": "month",
                "directionality": "higher_is_better",
            },
            {
                "kpi": "åˆ©æ¶¦ç‡/æ¯›åˆ©ç‡",
                "description": "åˆ©æ¶¦ç‡=åˆ©æ¶¦/æ”¶å…¥",
                "formula": "(revenue-cost)/revenue",
                "required_columns": ["revenue", "cost"],
                "group_by": [],
                "time_grain": "month",
                "directionality": "higher_is_better",
            },
        ],
        "trend_questions": [
            {"question": "æ”¶å…¥/æˆæœ¬/åˆ©æ¶¦çš„è¶‹åŠ¿ä¸æ³¢åŠ¨ï¼ˆç¯æ¯”/åŒæ¯”ï¼‰å¦‚ä½•ï¼Ÿ", "method": "æ—¶é—´åºåˆ— + ç¯æ¯”/åŒæ¯”", "required_columns": ["time", "revenue"]},
            {"question": "åˆ©æ¶¦ç‡å¼‚å¸¸æ³¢åŠ¨æ˜¯å¦æ¥è‡ªæˆæœ¬ç«¯æˆ–æ”¶å…¥ç«¯ï¼Ÿ", "method": "æ‹†åˆ†è´¡çŒ® + åˆ†ç»„å¯¹æ¯”", "required_columns": ["revenue", "cost"]},
        ],
        "anomaly_scan_plan": [
            {
                "name": "åˆ©æ¶¦ç‡å¼‚å¸¸ï¼ˆ>100%æˆ–<-100%ï¼‰",
                "method": "rule_based",
                "target_columns": ["revenue", "cost"],
                "threshold_or_rule": "åˆ©æ¶¦ç‡>1 æˆ– < -1",
                "flag": "å¾…äººå·¥å¤æ ¸",
            },
            {
                "name": "å¤§é¢äº¤æ˜“/å¼‚å¸¸æ³¢åŠ¨",
                "method": "IQR",
                "target_columns": ["revenue", "cost"],
                "threshold_or_rule": "é‡‘é¢åˆ— IQR æç«¯å€¼ + æ—¶é—´èšåˆçªå˜",
                "flag": "é‡ç‚¹å…³æ³¨åŒºåŸŸ",
            },
        ],
    },
    "marketing": {
        "kpi_definitions": [
            {
                "kpi": "æ›å…‰é‡",
                "description": "æ›å…‰æ€»é‡",
                "formula": "sum(impressions)",
                "required_columns": ["impressions"],
                "group_by": [],
                "time_grain": "day",
                "directionality": "higher_is_better",
            },
            {
                "kpi": "ç‚¹å‡»é‡",
                "description": "ç‚¹å‡»æ€»é‡",
                "formula": "sum(clicks)",
                "required_columns": ["clicks"],
                "group_by": [],
                "time_grain": "day",
                "directionality": "higher_is_better",
            },
            {
                "kpi": "CTR",
                "description": "ç‚¹å‡»ç‡=ç‚¹å‡»/æ›å…‰",
                "formula": "clicks/impressions",
                "required_columns": ["clicks", "impressions"],
                "group_by": [],
                "time_grain": "day",
                "directionality": "higher_is_better",
            },
            {
                "kpi": "è½¬åŒ–é‡",
                "description": "è½¬åŒ–æ€»é‡",
                "formula": "sum(conversions)",
                "required_columns": ["conversions"],
                "group_by": [],
                "time_grain": "day",
                "directionality": "higher_is_better",
            },
            {
                "kpi": "CVR",
                "description": "è½¬åŒ–ç‡=è½¬åŒ–/ç‚¹å‡»",
                "formula": "conversions/clicks",
                "required_columns": ["conversions", "clicks"],
                "group_by": [],
                "time_grain": "day",
                "directionality": "higher_is_better",
            },
            {
                "kpi": "CPA",
                "description": "è·å®¢æˆæœ¬=cost/conversions",
                "formula": "cost/conversions",
                "required_columns": ["cost", "conversions"],
                "group_by": [],
                "time_grain": "day",
                "directionality": "lower_is_better",
            },
        ],
        "trend_questions": [
            {"question": "å„æ¸ é“ CTR/CVR æ˜¯å¦ç¨³å®šï¼Ÿæ˜¯å¦å­˜åœ¨æ˜æ˜¾ä¸‹æ»‘ï¼Ÿ", "method": "åˆ†æ¸ é“æ—¶é—´åºåˆ— + å˜åŒ–ç‚¹", "required_columns": ["time"]},
            {"question": "æˆæœ¬æŠ•æ”¾å˜åŒ–æ˜¯å¦å¸¦æ¥è½¬åŒ–æå‡ï¼ˆROI/è¾¹é™…æ•ˆåº”ï¼‰ï¼Ÿ", "method": "ç›¸å…³æ€§/åˆ†æ®µå¯¹æ¯”", "required_columns": ["cost"]},
        ],
        "anomaly_scan_plan": [
            {
                "name": "æ¯”ç‡è¶Šç•Œï¼ˆCTR/CVR > 1ï¼‰",
                "method": "rule_based",
                "target_columns": ["clicks", "impressions", "conversions"],
                "threshold_or_rule": "clicks>impressions æˆ– conversions>clicks",
                "flag": "å¾…äººå·¥å¤æ ¸",
            }
        ],
    },
    "product": {
        "kpi_definitions": [
            {
                "kpi": "DAU",
                "description": "æ—¥æ´»ç”¨æˆ·æ•°",
                "formula": "nunique(user_id) by day",
                "required_columns": ["time", "user_id"],
                "group_by": [],
                "time_grain": "day",
                "directionality": "higher_is_better",
            },
            {
                "kpi": "ç•™å­˜ç‡ï¼ˆç²—ç•¥ï¼‰",
                "description": "æ¬¡æ—¥/7æ—¥ç•™å­˜ï¼ˆè‹¥èƒ½è¯†åˆ«ç”¨æˆ·ä¸æ—¶é—´ï¼‰",
                "formula": "retention = users_returned/users_base",
                "required_columns": ["time", "user_id"],
                "group_by": [],
                "time_grain": "day",
                "directionality": "higher_is_better",
            },
        ],
        "trend_questions": [
            {"question": "DAU æ˜¯å¦å¢é•¿ï¼Ÿå¢é•¿æ¥è‡ªæ–°ç”¨æˆ·è¿˜æ˜¯è€ç”¨æˆ·ï¼Ÿ", "method": "DAU æ—¶é—´åºåˆ— + æ–°è€ç”¨æˆ·æ‹†åˆ†", "required_columns": ["time", "user_id"]},
        ],
        "anomaly_scan_plan": [
            {
                "name": "DAU çªå˜/å¼‚å¸¸å³°å€¼",
                "method": "change_point",
                "target_columns": ["time", "user_id"],
                "threshold_or_rule": "æ—¥æ´»æ³¢åŠ¨è¶…å‡ºå†å²å‡å€¼Â±3Ïƒ æˆ– å˜åŒ–ç‚¹æ£€æµ‹",
                "flag": "é‡ç‚¹å…³æ³¨åŒºåŸŸ",
            }
        ],
    },
    "ops": {
        "kpi_definitions": [
            {
                "kpi": "äº‹ä»¶/å·¥å•é‡",
                "description": "äº‹ä»¶/å·¥å•æ•°é‡ï¼ˆè‹¥å­˜åœ¨ id åˆ—åˆ™å»é‡ï¼‰",
                "formula": "count or nunique(ticket_id)",
                "required_columns": ["ticket_id_or_event_id"],
                "group_by": [],
                "time_grain": "day",
                "directionality": "lower_is_better",
            },
            {
                "kpi": "å¤„ç†æ—¶é•¿",
                "description": "å¹³å‡å¤„ç†æ—¶é•¿/å“åº”æ—¶é•¿",
                "formula": "mean(duration)",
                "required_columns": ["duration"],
                "group_by": [],
                "time_grain": "day",
                "directionality": "lower_is_better",
            },
        ],
        "trend_questions": [
            {"question": "äº‹ä»¶é‡æ˜¯å¦é›†ä¸­åœ¨æŸäº›æ—¶é—´æ®µ/æœåŠ¡/æ¨¡å—ï¼Ÿ", "method": "åˆ†ç»„ TopN + è¶‹åŠ¿", "required_columns": ["time"]},
        ],
        "anomaly_scan_plan": [
            {
                "name": "å¤„ç†æ—¶é•¿å¼‚å¸¸ï¼ˆè¶…é•¿å°¾ï¼‰",
                "method": "IQR",
                "target_columns": ["duration"],
                "threshold_or_rule": "IQR æç«¯å€¼ + P95/P99",
                "flag": "é‡ç‚¹å…³æ³¨åŒºåŸŸ",
            }
        ],
    },
}


def _normalize_sop(sop: Dict[str, Any]) -> Dict[str, Any]:
    """
    å…œåº•ä¿®å¤ Stage1 SOPï¼šä¿è¯å…³é”®å­—æ®µå­˜åœ¨ï¼Œé¿å…åç»­ Stage2/3/4 å› ç¼ºå­—æ®µå´©æºƒã€‚
    """
    sop = sop or {}
    sop.setdefault("data_triage", {})
    triage = sop["data_triage"] if isinstance(sop["data_triage"], dict) else {}
    sop["data_triage"] = triage
    triage.setdefault("data_shape_hint", None)
    triage.setdefault("data_form", "unknown")
    triage.setdefault("domain_guess", "unknown")
    triage.setdefault("potential_time_columns", [])
    triage.setdefault("potential_id_columns", [])
    triage.setdefault("numeric_metric_columns", [])
    triage.setdefault("categorical_dimension_columns", [])
    triage.setdefault("text_columns", [])
    triage.setdefault("known_units_or_currency", {"unit": None, "currency": None})

    sop["noise_and_quality_issues"] = _ensure_list(sop.get("noise_and_quality_issues"))
    sop["cleaning_plan"] = _ensure_list(sop.get("cleaning_plan"))
    sop.setdefault("analysis_objective", {"user_goal": "", "success_criteria": []})
    if not isinstance(sop["analysis_objective"], dict):
        sop["analysis_objective"] = {"user_goal": "", "success_criteria": []}
    sop["analysis_objective"].setdefault("user_goal", "")
    sop["analysis_objective"]["success_criteria"] = _ensure_list(sop["analysis_objective"].get("success_criteria"))

    sop["kpi_definitions"] = _ensure_list(sop.get("kpi_definitions"))
    sop["trend_questions"] = _ensure_list(sop.get("trend_questions"))
    sop["anomaly_scan_plan"] = _ensure_list(sop.get("anomaly_scan_plan"))
    sop.setdefault("confidence_scoring_rules", {"scale": "0~1", "guideline": []})
    if not isinstance(sop["confidence_scoring_rules"], dict):
        sop["confidence_scoring_rules"] = {"scale": "0~1", "guideline": []}
    sop["confidence_scoring_rules"].setdefault("scale", "0~1")
    sop["confidence_scoring_rules"]["guideline"] = _ensure_list(sop["confidence_scoring_rules"].get("guideline"))

    sop.setdefault(
        "report_requirements",
        {
            "mandatory_sections": ["æ‰§è¡Œæ‘˜è¦", "æ–¹æ³•è®ºä¸æ•°æ®æ¦‚å†µ", "æ ¸å¿ƒæ•°æ®å‘ç°", "æ·±åº¦ä¸šåŠ¡æ´å¯Ÿ", "è¡ŒåŠ¨å»ºè®®", "é£é™©ä¸éœ€äººå·¥å¤æ ¸", "é™„å½•"],
            "must_reference_numbers": True,
            "output_format": "markdown",
        },
    )
    if not isinstance(sop["report_requirements"], dict):
        sop["report_requirements"] = {
            "mandatory_sections": ["æ‰§è¡Œæ‘˜è¦", "æ–¹æ³•è®ºä¸æ•°æ®æ¦‚å†µ", "æ ¸å¿ƒæ•°æ®å‘ç°", "æ·±åº¦ä¸šåŠ¡æ´å¯Ÿ", "è¡ŒåŠ¨å»ºè®®", "é£é™©ä¸éœ€äººå·¥å¤æ ¸", "é™„å½•"],
            "must_reference_numbers": True,
            "output_format": "markdown",
        }

    sop["notes"] = _ensure_list(sop.get("notes"))
    return sop


def _apply_domain_templates(sop: Dict[str, Any]) -> Dict[str, Any]:
    """
    å°†è¡Œä¸šæ¨¡æ¿â€œè¡¥é½â€åˆ° SOPï¼ˆåªåœ¨å†…å®¹ä¸è¶³æ—¶è¡¥å……ï¼Œé¿å…æ— è°“è†¨èƒ€ï¼‰ã€‚
    """
    triage = sop.get("data_triage", {}) if isinstance(sop.get("data_triage"), dict) else {}
    domain = str(triage.get("domain_guess") or "unknown").strip().lower()
    tmpl = DOMAIN_TEMPLATES.get(domain, DOMAIN_TEMPLATES["generic"])

    # KPIï¼šè‡³å°‘è¡¥åˆ° 6 ä¸ªï¼ˆgeneric è‡³å°‘ 3ï¼‰
    min_kpi = 6 if domain in DOMAIN_TEMPLATES and domain != "generic" and domain != "unknown" else 3
    existing_kpis = {str(x.get("kpi")).strip() for x in sop.get("kpi_definitions", []) if isinstance(x, dict) and x.get("kpi")}
    if len(existing_kpis) < min_kpi:
        for k in tmpl.get("kpi_definitions", []):
            name = str(k.get("kpi")).strip()
            if name and name not in existing_kpis:
                sop["kpi_definitions"].append(k)
                existing_kpis.add(name)
            if len(existing_kpis) >= min_kpi:
                break

    # è¶‹åŠ¿ï¼šè‡³å°‘ 2 æ¡
    if len(sop.get("trend_questions", [])) < 2:
        sop["trend_questions"].extend(tmpl.get("trend_questions", []))

    # å¼‚å¸¸ï¼šè‡³å°‘ 2 æ¡
    if len(sop.get("anomaly_scan_plan", [])) < 2:
        sop["anomaly_scan_plan"].extend(tmpl.get("anomaly_scan_plan", []))

    sop["notes"] = _ensure_list(sop.get("notes"))
    sop["notes"].append(f"applied_domain_template={domain if domain in DOMAIN_TEMPLATES else 'generic'}")
    return sop


def _normalize_insights(pkg: Dict[str, Any]) -> Dict[str, Any]:
    pkg = pkg or {}
    pkg["insights"] = _ensure_list(pkg.get("insights"))
    pkg["blind_spot_checks"] = _ensure_list(pkg.get("blind_spot_checks"))
    pkg["conflicts_or_suspicions"] = _ensure_list(pkg.get("conflicts_or_suspicions"))
    pkg["questions_to_user"] = _ensure_list(pkg.get("questions_to_user"))
    pkg["assumptions"] = _ensure_list(pkg.get("assumptions"))
    pkg["notes"] = _ensure_list(pkg.get("notes"))
    return pkg


def _normalize_stage4_output(judge: Dict[str, Any], hard_pkg: Dict[str, Any], insight_pkg: Dict[str, Any]) -> Dict[str, Any]:
    judge = judge or {}
    hard_findings = _ensure_list((hard_pkg or {}).get("hard_findings"))
    hard_by_id: Dict[str, Dict[str, Any]] = {}
    for hf in hard_findings:
        if isinstance(hf, dict) and hf.get("id"):
            hard_by_id[str(hf["id"])] = hf

    judge["executive_summary_bullets"] = _ensure_list(judge.get("executive_summary_bullets"))
    judge["merged_findings"] = _ensure_list(judge.get("merged_findings"))
    judge["conflict_resolution"] = _ensure_list(judge.get("conflict_resolution"))
    judge.setdefault("technical_summary_markdown", "")
    judge["open_issues"] = _ensure_list(judge.get("open_issues"))
    judge["recommended_next_steps"] = _ensure_list(judge.get("recommended_next_steps"))
    judge["chart_plan"] = _ensure_list(judge.get("chart_plan"))

    # report_finding_rowsï¼šä¼˜å…ˆç”± merged_findings ç”Ÿæˆ
    report_rows = _ensure_list(judge.get("report_finding_rows"))
    if not report_rows:
        for mf in judge["merged_findings"]:
            if not isinstance(mf, dict):
                continue
            mf_id = mf.get("id") or ""
            from_hard = _ensure_list(mf.get("from_hard"))
            review_tag = "OK"
            for hid in from_hard:
                hf = hard_by_id.get(str(hid))
                if not hf:
                    continue
                rt = str(hf.get("review_tag") or "OK")
                if rt == "å¾…äººå·¥å¤æ ¸":
                    review_tag = "å¾…äººå·¥å¤æ ¸"
                    break
                if rt == "é‡ç‚¹å…³æ³¨åŒºåŸŸ" and review_tag == "OK":
                    review_tag = "é‡ç‚¹å…³æ³¨åŒºåŸŸ"
            report_rows.append(
                {
                    "id": mf_id,
                    "final_statement": mf.get("final_statement", ""),
                    "confidence": mf.get("confidence", 0.5),
                    "review_tag": review_tag,
                    "status": mf.get("status", "TENTATIVE"),
                    "evidence_ids": from_hard,
                    "notes": mf.get("reason", ""),
                }
            )

    judge["report_finding_rows"] = report_rows

    # human_review_listï¼šæ±‡æ€»ç¡¬æ€§ç»“è®ºä¸­çš„å¤æ ¸é¡¹ + ç›²ç‚¹å®¡æŸ¥é¡¹
    human_review = _ensure_list(judge.get("human_review_list"))
    if not human_review:
        for hf in hard_findings:
            if not isinstance(hf, dict):
                continue
            rt = str(hf.get("review_tag") or "OK")
            if rt != "OK":
                human_review.append(
                    {
                        "related_id": hf.get("id", ""),
                        "issue": hf.get("title", "éœ€å¤æ ¸é¡¹"),
                        "why": hf.get("statement", ""),
                        "how_to_verify": hf.get("evidence", ""),
                    }
                )
        for bs in _ensure_list((insight_pkg or {}).get("blind_spot_checks")):
            if not isinstance(bs, dict):
                continue
            rel = _ensure_list(bs.get("related_hard_findings"))
            human_review.append(
                {
                    "related_id": rel[0] if rel else "",
                    "issue": bs.get("issue", ""),
                    "why": bs.get("why_suspicious", ""),
                    "how_to_verify": bs.get("how_to_verify", ""),
                }
            )
    judge["human_review_list"] = human_review
    return judge


PROMPTS: Dict[str, str] = {
    # Stage 1: é¡¹ç›®ç»ç†ï¼ˆæ™ºè°± GLMï¼‰
    "stage1_manager": """
ä½ æ˜¯ä¸€åã€æ•°æ®åˆ†æé¡¹ç›®ç»ç†ï¼ˆæ•°æ®åˆ†è¯Šå®˜ï¼‰ã€‘ã€‚
ä½ æ”¶åˆ°çš„æ˜¯â€œç”¨æˆ·åŸå§‹æ•°æ®ï¼ˆå¯èƒ½æ˜¯ CSV/JSON/æ–‡æœ¬/æ—¥å¿—/æ··åˆï¼‰+ ç”¨æˆ·ç®€è¦åˆ†æéœ€æ±‚â€ã€‚
ä½ çš„ç›®æ ‡ä¸æ˜¯ç«‹åˆ»ç»™ç»“è®ºï¼Œè€Œæ˜¯å…ˆåšã€åˆ†è¯Š + æ¸…æ´—å»ºè®® + SOP ä»»åŠ¡ä¹¦ã€‘ï¼Œè®©åç»­ç§‘å­¦å®¶/é¡¾é—®ä¸¥æ ¼æŒ‰ä»»åŠ¡ä¹¦æ‰§è¡Œï¼Œé¿å…æ¼«æ— ç›®çš„çš„â€œæ³›åˆ†æâ€ã€‚

ã€è¾“å…¥æ•°æ®ï¼ˆåŸå§‹/æ··åˆéƒ½å¯èƒ½ï¼‰ã€‘
{data_context}

ã€ç”¨æˆ·éœ€æ±‚ã€‘
{user_request}

ã€ä½ è¦å®Œæˆçš„å·¥ä½œã€‘
1) æ•°æ®åˆ†è¯Šä¸å™ªå£°è¯†åˆ«
- åˆ¤æ–­æ•°æ®å½¢æ€ï¼štable(ç»“æ„åŒ–è¡¨æ ¼)/time_series(æ—¶é—´åºåˆ—)/text(æ–‡æœ¬è¯„è®º)/log(æ—¥å¿—)/mixed(æ··åˆ)
- åˆ—/å­—æ®µè¯†åˆ«ï¼šå°½é‡æ¨æ–­å¯èƒ½çš„æ—¶é—´åˆ—ã€ä¸»é”®åˆ—ã€æŒ‡æ ‡åˆ—ï¼ˆæ•°å€¼åº¦é‡ï¼‰ã€ç»´åº¦åˆ—ï¼ˆç±»åˆ«/åœ°åŒº/æ¸ é“ç­‰ï¼‰ã€æ–‡æœ¬åˆ—
- æ˜ç¡®â€œæ˜æ˜¾æ ¼å¼å™ªå£°/è´¨é‡é£é™©â€ï¼šå¦‚ç¼ºå¤±/é‡å¤/ç¼–ç å¼‚å¸¸/å•ä½æˆ–å¸ç§æ··ä¹±/ç™¾åˆ†å·ä¸å°æ•°æ··ç”¨/å¼‚å¸¸åˆ†éš”ç¬¦/å­—æ®µå«ä¹‰ä¸æ¸…/æç«¯å€¼ç­‰
- ç»™å‡ºå¯æ‰§è¡Œçš„ cleaning_planï¼ˆæŒ‰æ­¥éª¤ã€è¯´æ˜åŸå› ã€ä¼˜å…ˆçº§ï¼‰

2) è¾“å‡ºä¸€ä»½ä¸¥æ ¼ JSON çš„ SOP ä»»åŠ¡ä¹¦ï¼ˆå¿…é¡»èƒ½è¢« json.loads è§£æï¼‰
SOP çš„ä½œç”¨ï¼šè®©åç»­é¦–å¸­ç§‘å­¦å®¶å¯ä»¥ç›´æ¥æŒ‰æ­¤å†™ä»£ç è®¡ç®— KPI/è¶‹åŠ¿/å¼‚å¸¸ï¼Œå¹¶äº§å‡ºâ€œä¸­é—´æ€æ•°æ®åŒ…â€ã€‚

ã€è¾“å‡ºè¦æ±‚ï¼ˆéå¸¸é‡è¦ï¼‰ã€‘
- åªè¾“å‡º 1 ä¸ª JSON å¯¹è±¡ï¼ˆä¸è¦ Markdownã€ä¸è¦å¤šä½™è§£é‡Šæ–‡å­—ï¼‰
- å­—æ®µå¿…é¡»é½å…¨ï¼Œå…è®¸å€¼ä¸º null/[]ï¼Œä½†ä¸è¦ç¼ºå­—æ®µ

ã€SOP JSON Schemaï¼ˆå­—æ®µå¿…é¡»å…¨éƒ¨å‡ºç°ï¼‰ã€‘
{{
  "data_triage": {{
    "data_shape_hint": "å¦‚æœèƒ½ä»è¾“å…¥æ¨æ–­å°±å¡«å†™ï¼Œå¦åˆ™å†™ null",
    "data_form": "table|time_series|text|log|mixed|unknown",
    "domain_guess": "sales|finance|ops|marketing|product|customer_service|other|unknown",
    "potential_time_columns": [],
    "potential_id_columns": [],
    "numeric_metric_columns": [],
    "categorical_dimension_columns": [],
    "text_columns": [],
    "known_units_or_currency": {{"unit": null, "currency": null}}
  }},
  "noise_and_quality_issues": [
    {{"issue": "...", "why_it_matters": "...", "how_to_check": "..."}}
  ],
  "cleaning_plan": [
    {{"step": "P0/P1/P2", "operation": "...(å¯æ‰§è¡Œæè¿°)", "why": "...", "expected_effect": "..."}}
  ],
  "analysis_objective": {{
    "user_goal": "...(å°†ç”¨æˆ·éœ€æ±‚ç»“æ„åŒ–æ”¹å†™ï¼›è‹¥ä¸ºç©ºåˆ™å†™ï¼šå…¨é¢EDA+ä¸šåŠ¡æ´å¯Ÿ+é£é™©æ‰«æ)",
    "success_criteria": ["...å¯è¡¡é‡æ ‡å‡†ï¼Œå¦‚ï¼šè¾“å‡ºTop5å¼‚å¸¸ç‚¹ã€ç»™å‡ºå…³é”®KPIè¡¨ç­‰"]
  }},
  "kpi_definitions": [
    {{
      "kpi": "KPIåç§°",
      "description": "ä¸šåŠ¡å«ä¹‰",
      "formula": "å°½é‡å†™æ¸…æ¥šï¼ˆå¯ç”¨è‡ªç„¶è¯­è¨€æˆ–ä¼ªå…¬å¼ï¼‰",
      "required_columns": [],
      "group_by": [],
      "time_grain": "none|day|week|month|quarter|year",
      "directionality": "higher_is_better|lower_is_better|unknown"
    }}
  ],
  "trend_questions": [
    {{"question": "...", "method": "åŒæ¯”/ç¯æ¯”/ç§»åŠ¨å¹³å‡/åˆ†ç»„å¯¹æ¯”/ç›¸å…³æ€§ç­‰", "required_columns": []}}
  ],
  "anomaly_scan_plan": [
    {{
      "name": "å¼‚å¸¸æ‰«æé¡¹åç§°",
      "method": "IQR|zscore|rule_based|change_point|schema_validation",
      "target_columns": [],
      "threshold_or_rule": "...(é˜ˆå€¼/è§„åˆ™)",
      "flag": "å¾…äººå·¥å¤æ ¸|é‡ç‚¹å…³æ³¨åŒºåŸŸ|æç¤º"
    }}
  ],
  "confidence_scoring_rules": {{
    "scale": "0~1ï¼Œè¶Šé«˜è¶Šå¯ä¿¡",
    "guideline": [
      "æ ·æœ¬é‡è¶Šå¤§ã€ç¼ºå¤±è¶Šå°‘ã€å®šä¹‰è¶Šæ˜ç¡® -> ç½®ä¿¡åº¦è¶Šé«˜",
      "åªåœ¨å±€éƒ¨æ ·æœ¬/ç¼ºå¤±ä¸¥é‡/å­—æ®µå«ä¹‰ä¸æ¸… -> ç½®ä¿¡åº¦é™ä½å¹¶æ ‡è®°ä¸ºå¾…å¤æ ¸"
    ]
  }},
  "report_requirements": {{
    "mandatory_sections": ["æ‰§è¡Œæ‘˜è¦", "æ–¹æ³•è®ºä¸æ•°æ®æ¦‚å†µ", "æ ¸å¿ƒæ•°æ®å‘ç°", "æ·±åº¦ä¸šåŠ¡æ´å¯Ÿ", "è¡ŒåŠ¨å»ºè®®", "é£é™©ä¸éœ€äººå·¥å¤æ ¸", "é™„å½•"],
    "must_reference_numbers": true,
    "output_format": "markdown"
  }},
  "notes": []
}}
""",
    # Stage 2: é¦–å¸­ç§‘å­¦å®¶ï¼ˆDeepSeekï¼‰
    "stage2_scientist_code": """
ä½ æ˜¯ä¸€åã€é¦–å¸­æ•°æ®ç§‘å­¦å®¶ã€‘ï¼ˆå Code/Mathï¼‰ã€‚

ä½ å°†æ”¶åˆ°ï¼š
- SOP ä»»åŠ¡ä¹¦ï¼ˆä¸¥æ ¼ JSONï¼‰
- æ•°æ®æ¦‚å†µï¼ˆå¯èƒ½æ¥è‡ª CSV/JSON/æ–‡æœ¬/æ··åˆçš„æ‘˜è¦ï¼‰
- è¿è¡Œç¯å¢ƒä¸­å·²å­˜åœ¨ DataFrame `df`

ä½ çš„ç›®æ ‡ä¸æ˜¯â€œå†™æŠ¥å‘Šâ€ï¼Œè€Œæ˜¯ç”¨ä»£ç äº§å‡ºå¯å¤ç”¨çš„ã€ä¸­é—´æ€æ•°æ®åŒ…ã€‘ï¼ˆHard Findings Packageï¼‰ã€‚

ã€SOPï¼ˆJSONï¼‰ã€‘
{sop_json}

ã€æ•°æ®æ¦‚å†µã€‘
{data_context}

ã€è¿è¡Œç¯å¢ƒè¯´æ˜ã€‘
- å·²æœ‰å˜é‡ï¼šdfï¼ˆPandas DataFrameï¼‰
- å¯ç”¨ï¼špandas as pd, numpy as npï¼ˆå¯è‡ªè¡Œ importï¼‰, matplotlib.pyplot as plt, seaborn as sns, json
- ç¦æ­¢ï¼šæ–‡ä»¶è¯»å†™ã€ç½‘ç»œè¯·æ±‚ã€os æ“ä½œã€plt.show()

ã€åˆ—åè¯­ä¹‰åŒ¹é…ï¼ˆéå¸¸é‡è¦ï¼‰ã€‘
å¦‚æœ SOP çš„ required_columns åœ¨ df.columns ä¸­æ‰¾ä¸åˆ°ï¼Œä½ å¿…é¡»å…ˆå°è¯•â€œè¯­ä¹‰/å…³é”®è¯/æ¨¡ç³ŠåŒ¹é…â€å®šä½å€™é€‰åˆ—ï¼Œå†å†³å®šèƒ½å¦è®¡ç®—ï¼š
- æ—¶é—´åˆ—ï¼šdate, day, time, datetime, timestamp, dt, æ—¥æœŸ, æ—¶é—´
- é‡‘é¢/é”€å”®é¢ï¼šamount, revenue, sales, gmv, pay, äº¤æ˜“é¢, æ”¯ä»˜é‡‘é¢, é”€å”®é¢, é‡‘é¢, æˆäº¤é¢
- è®¢å•ï¼šorder, order_id, è®¢å•, trade, bill
- ç”¨æˆ·ï¼šuser, user_id, uid, customer, buyer, ç”¨æˆ·
- æˆæœ¬/è´¹ç”¨ï¼šcost, expense, spend, è´¹ç”¨, æˆæœ¬
- åˆ©æ¶¦ï¼šprofit, margin, åˆ©æ¶¦, æ¯›åˆ©
- æ•°é‡ï¼šqty, quantity, count, num, æ•°é‡
- æ›å…‰/ç‚¹å‡»ï¼šimpression, pv, uv, view, æ›å…‰, å±•ç°ï¼›click, ctr, ç‚¹å‡»
åŒ¹é…åè¯·æŠŠâ€œå®é™…å‘½ä¸­çš„åˆ—åâ€å†™å…¥ evidence/metricsï¼Œé¿å…å£å¾„ä¸æ¸…ã€‚

ã€ä½ å¿…é¡»äº§å‡ºçš„ä¸­é—´æ€æ•°æ®åŒ… JSON Schemaï¼ˆprint è¾“å‡ºï¼‰ã€‘
è¯·åœ¨ä»£ç ä¸­æ„é€ ä¸€ä¸ª dictï¼šmid = {{
  "meta": {{"generated_by": "deepseek", "assumptions": [], "limits": []}},
  "data_quality": {{
    "rows": int, "cols": int,
    "missing_top": [{{"column": str, "missing_pct": float}}],
    "duplicate_rows": int,
    "type_issues": [],
    "notes": []
  }},
  "hard_findings": [
    {{
      "id": "HF001",
      "category": "KPI|Trend|Correlation|Anomaly|LogicBreak|DataQuality",
      "title": "...",
      "statement": "...ï¼ˆå¿…é¡»åŒ…å«å…³é”®æ•°å€¼/æ¯”ä¾‹/èŒƒå›´ï¼‰",
      "metrics": {{}},
      "confidence": 0.00,
      "evidence": "...ï¼ˆåˆ—/åˆ†ç»„/æ—¶é—´èŒƒå›´/æ ·æœ¬é‡n/ç»Ÿè®¡å£å¾„ï¼‰",
      "review_tag": "OK|å¾…äººå·¥å¤æ ¸|é‡ç‚¹å…³æ³¨åŒºåŸŸ",
      "severity": "low|medium|high"
    }}
  ],
  "chart_suggestions": [{{"title": "...", "chart": "line|bar|heatmap|boxplot|scatter|table", "x": "...", "y": "...", "why": "..."}}]
}}

ã€å…³é”®è¦æ±‚ï¼ˆéå¸¸é‡è¦ï¼‰ã€‘
1) ä½ å¿…é¡»â€œæŒ‰ SOPâ€å°è¯•è®¡ç®— KPI/è¶‹åŠ¿/å¼‚å¸¸ï¼›è‹¥å­—æ®µç¼ºå¤±å¯¼è‡´æ— æ³•è®¡ç®—ï¼Œä¹Ÿè¦ç”Ÿæˆä¸€ä¸ª hard_findingï¼ˆreview_tag=å¾…äººå·¥å¤æ ¸ï¼Œconfidence<=0.3ï¼‰è¯´æ˜ç¼ºä»€ä¹ˆå­—æ®µã€‚
2) æ¯æ¡ hard_finding å¿…é¡»å¸¦ confidenceï¼ˆ0~1ï¼‰ï¼Œå¹¶è¯´æ˜è¯æ®ä¸å£å¾„ã€‚ç½®ä¿¡åº¦ä¸è¦å…¨ç»™ 1ã€‚
3) å¼‚å¸¸æ£€æµ‹å¿…é¡»è¦†ç›–ï¼š
   - æ•°å€¼åˆ—ï¼šIQR æˆ– z-score æ–¹å¼æ‰¾æç«¯å€¼ï¼ˆåªä¿ç•™ TopNï¼Œä¾‹å¦‚ 10 æ¡ï¼‰
   - é€»è¾‘æ–­å±‚ï¼šå¦‚è´Ÿé”€é‡/è´Ÿäººæ•°/åˆ©æ¶¦ç‡>100% ç­‰â€œç–‘ä¼¼ä¸åˆç†â€ç‚¹ï¼Œæ ‡è®° review_tag=å¾…äººå·¥å¤æ ¸ æˆ– é‡ç‚¹å…³æ³¨åŒºåŸŸ
4) è¾“å‡ºå¤§å°æ§åˆ¶ï¼šhard_findings æœ€å¤š 30 æ¡ï¼›missing_top æœ€å¤š 15ï¼›chart_suggestions æœ€å¤š 8
5) ä»£ç æœ€åå¿…é¡»åª print ä¸€ä»½ JSONï¼ˆmidï¼‰ï¼Œå°½é‡ä¸è¦ print å…¶å®ƒå†…å®¹ï¼š
   print(json.dumps(mid, ensure_ascii=False, indent=2))

ã€è¾“å‡ºæ ¼å¼ã€‘
- åªè¾“å‡ºä¸€ä¸ª ```python ä»£ç å—
""",
    # Stage 3: ä¸šåŠ¡é¡¾é—®ï¼ˆDeepSeek-Bï¼‰
    "stage3_consultant": """
ä½ æ˜¯ä¸€åã€èµ„æ·±ä¸šåŠ¡é¡¾é—®ã€‘ï¼ˆæ“…é•¿å½’å› ã€æ¨ªå‘çŸ¥è¯†åº“ã€é£é™©è¯†åˆ«ï¼‰ã€‚

ä½ å°†æ”¶åˆ°ï¼š
- åŸå§‹æ•°æ®æ¦‚å†µ/ç‰‡æ®µï¼ˆå¯èƒ½æ˜¯è¡¨æ ¼+æ–‡æœ¬æ··åˆï¼‰
- é¦–å¸­ç§‘å­¦å®¶è®¡ç®—å¾—åˆ°çš„â€œä¸­é—´æ€æ•°æ®åŒ…â€ï¼ˆç¡¬æ€§ç»“è®º JSONï¼Œå« confidence/review_tagï¼‰

ä½ çš„ä»»åŠ¡ï¼š
1) è¯­ä¹‰æŒ–æ˜ä¸å½’å› ï¼šè§£é‡Šâ€œä¸ºä»€ä¹ˆä¼šè¿™æ ·â€ï¼ˆç»“åˆä¸šåŠ¡å¸¸è¯†/èŠ‚å‡æ—¥/ä¿ƒé”€/å­£èŠ‚æ€§/æ¸ é“å˜åŒ–ç­‰ï¼‰ï¼Œä½†å¿…é¡»é”šå®šç¡¬ç»“è®ºï¼ˆå¼•ç”¨ hard_findings çš„ idï¼‰ã€‚
2) å¤šç»´åº¦è§†è§’è¡¥å……ï¼šä»å¸‚åœºã€ç”¨æˆ·å¿ƒç†ã€æ½œåœ¨é£é™©ã€è¿è¥åŠ¨ä½œç­‰ç»´åº¦å‘æ•£ï¼Œå½¢æˆå¯æ‰§è¡Œå»ºè®®ã€‚
3) ç›²ç‚¹å®¡æŸ¥ï¼ˆç¬¬ä¸€å±‚æ ¡éªŒï¼‰ï¼šæŒ‡å‡ºç¡¬ç»“è®ºä¸­å¯èƒ½çš„ä¸šåŠ¡çŸ›ç›¾/ä¸å¯ç½®ä¿¡ç‚¹ï¼ˆä¾‹å¦‚åˆ©æ¶¦ç‡>100%ã€é”€é‡ä¸ºè´Ÿç­‰ï¼‰ï¼Œç»™å‡ºâ€œå¦‚ä½•éªŒè¯â€çš„å»ºè®®ã€‚

ã€è¾“å…¥ï¼šåŸå§‹æ•°æ®æ¦‚å†µ/ç‰‡æ®µã€‘
{data_context}

ã€è¾“å…¥ï¼šä¸­é—´æ€æ•°æ®åŒ…ï¼ˆJSONï¼‰ã€‘
{hard_package_json}

ã€è¾“å‡ºè¦æ±‚ï¼ˆéå¸¸é‡è¦ï¼‰ã€‘
- åªè¾“å‡º 1 ä¸ª JSON å¯¹è±¡ï¼ˆä¸è¦ Markdownï¼Œä¸è¦è§£é‡Šæ–‡å­—ï¼‰
- JSON å¿…é¡»èƒ½è¢« json.loads è§£æ
- æ¯æ¡æ´å¯Ÿå¿…é¡»å¼•ç”¨ç›¸å…³ hard_findings çš„ idï¼ˆbased_onï¼‰

ã€è¾“å‡º JSON Schemaï¼ˆå­—æ®µå¿…é¡»å…¨éƒ¨å‡ºç°ï¼‰ã€‘
{{
  "insights": [
    {{
      "id": "I01",
      "title": "...",
      "why": "...ï¼ˆå½’å› æ¨ç†ï¼‰",
      "based_on": ["HF001"],
      "confidence": 0.00,
      "actions": ["...å¯æ‰§è¡ŒåŠ¨ä½œ"],
      "risks": ["...æ½œåœ¨é£é™©"]
    }}
  ],
  "blind_spot_checks": [
    {{
      "issue": "...ï¼ˆå¯èƒ½çŸ›ç›¾/å¼‚å¸¸çš„ç‚¹ï¼‰",
      "related_hard_findings": ["HF001"],
      "severity": "low|medium|high",
      "why_suspicious": "...",
      "how_to_verify": "...ï¼ˆä¸‹ä¸€æ­¥å¦‚ä½•ç”¨æ•°æ®/ä¸šåŠ¡æ ¸éªŒï¼‰"
    }}
  ],
  "conflicts_or_suspicions": [
    {{"description": "...", "related_hard_findings": ["HF001"], "suggestion": "..."}}
  ],
  "questions_to_user": [],
  "assumptions": [],
  "notes": []
}}
""",
    # Stage 4: é¦–å¸­ç§‘å­¦å®¶å›å½’ï¼ˆDeepSeekï¼‰
    "stage4_scientist_judge": """
ä½ æ˜¯ä¸€åã€é¦–å¸­æ•°æ®ç§‘å­¦å®¶ï¼ˆè£åˆ¤/æ”¶æ•›è€…ï¼‰ã€‘ã€‚

ä½ å°†æ”¶åˆ°ï¼š
- SOP ä»»åŠ¡ä¹¦ï¼ˆJSONï¼‰
- ä¸­é—´æ€æ•°æ®åŒ…ï¼ˆç¡¬æ€§ç»“è®º JSONï¼šhard_findingsï¼Œå« confidence/review_tagï¼‰
- ä¸šåŠ¡é¡¾é—®æ´å¯ŸåŒ…ï¼ˆJSONï¼šinsights + blind_spot_checks + conflictsï¼‰

ä½ çš„ä»»åŠ¡ï¼š
1) é€»è¾‘æ”¶æ•›ï¼šåˆå¹¶ç¡¬æ•°æ®ä¸è½¯æ´å¯Ÿã€‚è‹¥å†²çªï¼š
   - é»˜è®¤â€œæ•°æ®ä¼˜å…ˆâ€ï¼Œé™¤éç¡¬ç»“è®ºç½®ä¿¡åº¦å¾ˆä½æˆ–è¢«ç›²ç‚¹å®¡æŸ¥æŒ‡å‡ºé‡å¤§çŸ›ç›¾
   - éœ€è¦æ˜ç¡®ç»™å‡ºåŠ æƒç†ç”±ï¼ˆhard vs soft çš„æƒé‡ï¼‰
   - å…è®¸ä¿ç•™äº‰è®®ï¼šæ ‡è®°ä¸º DISPUTEDï¼Œå¹¶åˆ—å‡ºéœ€è¡¥å……çš„æ•°æ®/æ ¸éªŒæ–¹å¼
2) ç”Ÿæˆâ€œæŠ€æœ¯æ€§æ‘˜è¦â€ï¼ˆç”¨äºä¸»ç¬”æˆæ–‡ï¼‰ï¼šå¿…é¡»åŒ…å«ä¸¥è°¨é€»è¾‘é“¾æ¡ä¸å…³é”®æ•°å­—å¼•ç”¨ï¼ˆå¼•ç”¨ hard_findings çš„ idï¼‰

ã€è¾“å…¥ï¼šSOPï¼ˆJSONï¼‰ã€‘
{sop_json}

ã€è¾“å…¥ï¼šç¡¬æ€§ç»“è®ºï¼ˆJSONï¼‰ã€‘
{hard_package_json}

ã€è¾“å…¥ï¼šè½¯æ´å¯Ÿï¼ˆJSONï¼‰ã€‘
{insight_package_json}

ã€è¾“å‡ºè¦æ±‚ï¼ˆéå¸¸é‡è¦ï¼‰ã€‘
- åªè¾“å‡º 1 ä¸ª JSON å¯¹è±¡ï¼ˆä¸è¦ Markdownï¼Œä¸è¦è§£é‡Šæ–‡å­—ï¼‰
- JSON å¿…é¡»èƒ½è¢« json.loads è§£æ

ã€è¾“å‡º JSON Schemaï¼ˆå­—æ®µå¿…é¡»å…¨éƒ¨å‡ºç°ï¼‰ã€‘
{{
  "executive_summary_bullets": [],
  "merged_findings": [
    {{
      "id": "MF01",
      "from_hard": ["HF001"],
      "from_insight": ["I01"],
      "final_statement": "...ï¼ˆå¿…é¡»åŒ…å«å…³é”®æ•°å€¼ï¼›å¹¶å¼•ç”¨HFï¼‰",
      "confidence": 0.00,
      "status": "ACCEPTED|TENTATIVE|DISPUTED",
      "reason": "ä¸ºä½•è¿™æ ·è£å†³ï¼ˆå«æƒé‡é€»è¾‘ï¼‰"
    }}
  ],
  "conflict_resolution": [
    {{
      "topic": "...",
      "hard_side": "...",
      "soft_side": "...",
      "decision": "...",
      "weighting": {{"hard": 0.7, "soft": 0.3}},
      "follow_up": "è‹¥ä»æœ‰äº‰è®®ï¼Œå¦‚ä½•æ ¸éªŒ"
    }}
  ],
  "report_finding_rows": [
    {{
      "id": "MF01",
      "final_statement": "...ï¼ˆæŠ¥å‘Šä¸­è¦å±•ç¤ºçš„æœ€ç»ˆç»“è®ºï¼ŒåŒ…å«å…³é”®æ•°å€¼ï¼‰",
      "confidence": 0.00,
      "review_tag": "OK|å¾…äººå·¥å¤æ ¸|é‡ç‚¹å…³æ³¨åŒºåŸŸ",
      "status": "ACCEPTED|TENTATIVE|DISPUTED",
      "evidence_ids": ["HF001"],
      "notes": "è¡¥å……å£å¾„/æ ·æœ¬é‡/é™åˆ¶"
    }}
  ],
  "human_review_list": [
    {{
      "related_id": "HF001",
      "issue": "...ï¼ˆéœ€è¦äººå·¥å¤æ ¸çš„ç‚¹ï¼‰",
      "why": "...",
      "how_to_verify": "å¦‚ä½•æ ¸éªŒ/éœ€è¦è¡¥å……ä»€ä¹ˆå­—æ®µæˆ–ä¸šåŠ¡ä¿¡æ¯"
    }}
  ],
  "technical_summary_markdown": "...ï¼ˆç”¨äºå†™æŠ¥å‘Šçš„éª¨æ¶ï¼Œå…è®¸ Markdownï¼Œä½†æ”¾åœ¨è¿™ä¸ªå­—æ®µé‡Œï¼‰",
  "open_issues": [],
  "recommended_next_steps": [],
  "chart_plan": []
}}
""",
    # Stage 5: ä¸»ç¬”ï¼ˆæ™ºè°± GLMï¼‰
    "stage5_writer": """
ä½ æ˜¯ä¸€åã€ä¸“ä¸šå•†ä¸šåˆ†ææŠ¥å‘Šä¸»ç¬”ã€‘ï¼ˆæ“…é•¿ç»“æ„åŒ–å†™ä½œä¸ Markdown æ’ç‰ˆï¼‰ã€‚

ä½ å°†æ”¶åˆ°ï¼š
- SOP ä»»åŠ¡ä¹¦ï¼ˆJSONï¼‰
- æŠ€æœ¯æ€§æ‘˜è¦ï¼ˆJSONï¼Œå…¶ä¸­ technical_summary_markdown æ˜¯æŠ¥å‘Šéª¨æ¶ï¼‰

ä½ çš„ä»»åŠ¡ï¼š
1) æŠŠææ–™å†™æˆä¸€ç¯‡å®Œæ•´ã€è¿è´¯ã€å¯äº¤ä»˜çš„å•†ä¸šåˆ†ææŠ¥å‘Šï¼ˆMarkdownï¼‰ã€‚
2) ä¸¥æ ¼æŒ‰ç…§ç»“æ„è¾“å‡ºï¼š
   - æ‰§è¡Œæ‘˜è¦
   - æ–¹æ³•è®ºä¸æ•°æ®æ¦‚å†µ
   - æ ¸å¿ƒæ•°æ®å‘ç°ï¼ˆå¿…é¡»ç”¨è¡¨æ ¼æ±‡æ€»å…³é”®å‘ç°ï¼šID/ç»“è®º/ç½®ä¿¡åº¦/review_tagï¼‰
   - æ·±åº¦ä¸šåŠ¡æ´å¯Ÿ
   - è¡ŒåŠ¨å»ºè®®ï¼ˆåˆ† P0/P1/P2 ä¼˜å…ˆçº§ï¼Œå°½é‡é‡åŒ–ï¼‰
   - é£é™©ä¸éœ€äººå·¥å¤æ ¸ï¼ˆæŠŠ review_tag!=OK çš„é¡¹åˆ—å‡ºæ¥ï¼Œå¹¶ç»™å‡ºæ ¸éªŒæ–¹å¼ï¼‰
   - é™„å½•ï¼ˆé™„ä¸Š SOP JSON çš„ä»£ç å— + æœ¯è¯­/å£å¾„è¯´æ˜ï¼‰

ã€éå¸¸é‡è¦çš„å†™ä½œçº¦æŸã€‘
- ä¸è¦æœæ’°ä»»ä½•æ•°å­—ï¼›æ‰€æœ‰æ•°å­—å¿…é¡»æ¥è‡ª tech_summary_json ä¸­çš„ merged_findings/report_finding_rowsï¼ˆå¹¶ç”¨ ID å¼•ç”¨ï¼‰
- å¦‚æœç¼ºå°‘å¿…è¦æ•°å­—ï¼Œæ˜ç¡®å†™â€œæ•°æ®ä¸è¶³/å­—æ®µç¼ºå¤±â€ï¼Œå¹¶åˆ—å‡ºéœ€è¦è¡¥å……ä»€ä¹ˆ
- è¯­è¨€å®¢è§‚ã€å¯å®¡è®¡ï¼›ç»“è®ºè¦ä¸è¯æ®å¯¹åº”

ã€å†™ä½œæç¤ºã€‘
- â€œæ ¸å¿ƒæ•°æ®å‘ç°â€çš„æ±‡æ€»è¡¨æ ¼ä¼˜å…ˆä½¿ç”¨ tech_summary_json.report_finding_rows

ã€è¾“å…¥ï¼šSOPï¼ˆJSONï¼‰ã€‘
{sop_json}

ã€è¾“å…¥ï¼šæŠ€æœ¯æ€§æ‘˜è¦ï¼ˆJSONï¼‰ã€‘
{tech_summary_json}

ã€è¾“å‡ºæ ¼å¼ã€‘
- åªè¾“å‡º Markdown æ­£æ–‡ï¼ˆä¸è¦ JSONï¼‰
""",
}


def run_report_engine(
    *,
    user_request: str,
    data_context: str,
    api_keys: Dict[str, str],
    model_config: Dict[str, Any],
    execute_callback,
    df,
) -> Dict[str, Any]:
    """
    äº”é˜¶æ®µæŠ¥å‘Šç”Ÿæˆå¼•æ“å…¥å£ï¼ˆä¾› workflow_report.py è°ƒç”¨ï¼‰
    """
    roles = {"manager": "zhipu", "scientist": "deepseekA", "consultant": "qwen", "writer": "zhipu"}

    available_keys = [k for k, v in api_keys.items() if v]
    if not available_keys:
        return {"error": "æœªé…ç½® API Key", "log": "âŒ æ—  Key"}
    for r in roles:
        if not api_keys.get(roles[r]):
            roles[r] = available_keys[0]

    process_log: List[str] = []
    user_req = (user_request or "").strip() or "è¿›è¡Œå…¨é¢çš„æ¢ç´¢æ€§æ•°æ®åˆ†æï¼Œå¹¶è¾“å‡ºå¯æ‰§è¡Œçš„å•†ä¸šåˆ†ææŠ¥å‘Š"

    # --- Stage 1 ---
    process_log.append("### ğŸš© é˜¶æ®µä¸€ï¼šæ•°æ®é¢„å¤„ç†ä¸ä»»åŠ¡æ‹†è§£ï¼ˆé¡¹ç›®ç»ç†ï¼šDeepSeek-Cï¼‰")
    process_log.append("æ­£åœ¨è¿›è¡Œæ•°æ®åˆ†è¯Šã€å™ªå£°è¯†åˆ«ä¸ SOP åˆ¶å®š...")
    sop_prompt = PROMPTS["stage1_manager"].format(data_context=data_context, user_request=user_req)
    sop_result = _call_llm_expect_json_dict(
        roles["manager"],
        api_keys[roles["manager"]],
        model_config,
        sop_prompt,
        stage_name="Stage1/SOP",
        temperature=0.1,
        timeout=120,
        retries=2,
    )
    if "error" in sop_result:
        raw = sop_result.get("raw")
        if raw:
            process_log.append("\n### âš ï¸ Stage1 åŸå§‹è¾“å‡ºï¼ˆæˆªæ–­ï¼‰")
            process_log.append(f"```text\n{raw}\n```")
        return {"error": sop_result["error"], "log": "\n".join(process_log)}

    sop_obj = sop_result.get("obj") if isinstance(sop_result.get("obj"), dict) else {}
    sop_obj = _normalize_sop(sop_obj)
    sop_obj = _apply_domain_templates(sop_obj)
    sop_for_next = _json_dumps(sop_obj)
    process_log.append("> **SOPï¼ˆJSONï¼‰**ï¼š")
    process_log.append(f"```json\n{sop_for_next[:8000]}\n```")

    # --- Stage 2 ---
    process_log.append("\n### ğŸ§® é˜¶æ®µäºŒï¼šç¡¬æ ¸é€»è¾‘åˆ†æä¸è®¡ç®—ï¼ˆé¦–å¸­ç§‘å­¦å®¶ï¼šDeepSeekï¼‰")
    process_log.append("æ­£åœ¨ç”Ÿæˆ Python åˆ†æä»£ç ï¼ˆå°†äº§å‡ºä¸­é—´æ€æ•°æ®åŒ… JSONï¼‰...")
    code_prompt = PROMPTS["stage2_scientist_code"].format(sop_json=sop_for_next, data_context=data_context)

    hard_obj: Optional[Dict[str, Any]] = None
    hard_for_next = ""
    last_exec_text = ""

    for attempt in range(2):
        code_res = _call_llm(
            roles["scientist"],
            api_keys[roles["scientist"]],
            model_config,
            code_prompt,
            temperature=0.1,
            timeout=120,
            retries=2,
        )
        if code_res.startswith("Error"):
            return {"error": code_res, "log": "\n".join(process_log)}

        code = _extract_python_code(code_res)
        process_log.append("> **Stage2 ç”Ÿæˆä»£ç ï¼ˆæˆªæ–­é¢„è§ˆï¼‰**ï¼š")
        process_log.append(f"```python\n{code[:2500]}\n```")
        process_log.append("âš™ï¸ ç³»ç»Ÿæ­£åœ¨æ²™ç›’ä¸­æ‰§è¡Œåˆ†æä»£ç ...")
        exec_text, _, _ = execute_callback(code, df)
        last_exec_text = str(exec_text)

        if isinstance(exec_text, str) and (exec_text.startswith("Error") or "Traceback" in exec_text):
            # å…è®¸ 1 æ¬¡é‡è¯•ï¼šæŠŠæŠ¥é”™ç‰‡æ®µåé¦ˆç»™æ¨¡å‹ä¿®å¤
            if attempt == 0:
                code_prompt = (
                    code_prompt
                    + f"\n\n[ç³»ç»Ÿçº é”™] ä½ ä¸Šæ¬¡ä»£ç æ‰§è¡ŒæŠ¥é”™ï¼š{exec_text[:600]}\n"
                    + "è¯·ä¿®å¤ä»£ç ï¼Œå¹¶ç¡®ä¿æœ€ååª print ä¸€ä»½å¯è¢« json.loads è§£æçš„ä¸­é—´æ€æ•°æ®åŒ… JSONï¼ˆmidï¼‰ã€‚"
                )
                continue
            process_log.append(f"âŒ ä»£ç æ‰§è¡Œå¤±è´¥ï¼š{exec_text[:500]}")
            return {"error": "Stage2 åˆ†æä»£ç æ‰§è¡Œå¤±è´¥", "log": "\n".join(process_log)}

        hard_json_text = _extract_json_candidate(str(exec_text)) or ""
        parsed = _safe_json_loads(hard_json_text) if hard_json_text else None
        if isinstance(parsed, dict):
            hard_obj = parsed
            hard_for_next = _json_dumps(hard_obj)
            break

        # æ²¡æ‹¿åˆ°å¯è§£æ JSONï¼Œå…è®¸ 1 æ¬¡é‡è¯•
        if attempt == 0:
            code_prompt = (
                code_prompt
                + f"\n\n[ç³»ç»Ÿçº é”™] ä½ ä¸Šæ¬¡ä»£ç è¿è¡Œåæ²¡æœ‰æ‰“å°å‡ºå¯è§£æ JSONã€‚è¿è¡Œè¾“å‡ºç‰‡æ®µï¼š{last_exec_text[:600]}\n"
                + "è¯·ä¿®å¤ï¼šç¡®ä¿ä»£ç æœ€ååª print(json.dumps(mid, ensure_ascii=False, indent=2))ï¼Œä¸” mid æ»¡è¶³ Schemaã€‚"
            )

    if hard_obj is None:
        process_log.append(f"âŒ æœªæ‹¿åˆ°å¯è§£æçš„ä¸­é—´æ€æ•°æ®åŒ… JSONã€‚è¾“å‡ºç‰‡æ®µï¼š{last_exec_text[:600]}")
        return {"error": "Stage2 æœªè¿”å›æœ‰æ•ˆçš„ä¸­é—´æ€æ•°æ®åŒ… JSON", "log": "\n".join(process_log)}

    process_log.append("> **ä¸­é—´æ€æ•°æ®åŒ…ï¼ˆHard Findings Packageï¼Œæˆªæ–­é¢„è§ˆï¼‰**ï¼š")
    process_log.append(f"```json\n{hard_for_next[:8000]}\n```")

    # --- Stage 3 ---
    process_log.append("\n### ğŸ’¡ é˜¶æ®µä¸‰ï¼šä¸šåŠ¡æ´å¯Ÿä¸æ¨ªå‘å…³è”ï¼ˆä¸šåŠ¡é¡¾é—®ï¼šDeepSeek-Bï¼‰")
    process_log.append("æ­£åœ¨è¿›è¡Œå½’å› ã€å‘æ•£æ´å¯Ÿä¸ç›²ç‚¹å®¡æŸ¥...")
    insight_prompt = PROMPTS["stage3_consultant"].format(data_context=data_context, hard_package_json=hard_for_next)
    insight_result = _call_llm_expect_json_dict(
        roles["consultant"],
        api_keys[roles["consultant"]],
        model_config,
        insight_prompt,
        stage_name="Stage3/æ´å¯ŸåŒ…",
        temperature=0.3,
        timeout=120,
        retries=2,
    )
    if "error" in insight_result:
        raw = insight_result.get("raw")
        if raw:
            process_log.append("\n### âš ï¸ Stage3 åŸå§‹è¾“å‡ºï¼ˆæˆªæ–­ï¼‰")
            process_log.append(f"```text\n{raw}\n```")
        return {"error": insight_result["error"], "log": "\n".join(process_log)}

    insight_obj = insight_result.get("obj") if isinstance(insight_result.get("obj"), dict) else {}
    insight_obj = _normalize_insights(insight_obj)
    insight_for_next = _json_dumps(insight_obj)
    process_log.append("> **æ´å¯Ÿå»ºè®®åˆ—è¡¨ï¼ˆJSONï¼Œæˆªæ–­é¢„è§ˆï¼‰**ï¼š")
    process_log.append(f"```json\n{insight_for_next[:8000]}\n```")

    # --- Stage 4 ---
    process_log.append("\n### âš–ï¸ é˜¶æ®µå››ï¼šå†²çªè§£å†³ä¸æ·±åº¦ç»¼è¿°ï¼ˆé¦–å¸­ç§‘å­¦å®¶ï¼šDeepSeekï¼‰")
    process_log.append("æ­£åœ¨åˆå¹¶ç¡¬æ•°æ®ä¸è½¯æ´å¯Ÿï¼Œè¿›è¡Œè£å†³ä¸é€»è¾‘æ”¶æ•›...")
    judge_prompt = PROMPTS["stage4_scientist_judge"].format(
        sop_json=sop_for_next,
        hard_package_json=hard_for_next,
        insight_package_json=insight_for_next,
    )
    judge_result = _call_llm_expect_json_dict(
        roles["scientist"],
        api_keys[roles["scientist"]],
        model_config,
        judge_prompt,
        stage_name="Stage4/æŠ€æœ¯æ€§æ‘˜è¦",
        temperature=0.1,
        timeout=120,
        retries=2,
    )
    if "error" in judge_result:
        raw = judge_result.get("raw")
        if raw:
            process_log.append("\n### âš ï¸ Stage4 åŸå§‹è¾“å‡ºï¼ˆæˆªæ–­ï¼‰")
            process_log.append(f"```text\n{raw}\n```")
        return {"error": judge_result["error"], "log": "\n".join(process_log)}

    judge_obj = judge_result.get("obj") if isinstance(judge_result.get("obj"), dict) else {}
    judge_obj = _normalize_stage4_output(judge_obj, hard_obj or {}, insight_obj)
    judge_for_next = _json_dumps(judge_obj)
    process_log.append("> **æŠ€æœ¯æ€§æ‘˜è¦ï¼ˆJSONï¼Œæˆªæ–­é¢„è§ˆï¼‰**ï¼š")
    process_log.append(f"```json\n{judge_for_next[:8000]}\n```")

    # --- Stage 5 ---
    process_log.append("\n### ğŸ“ é˜¶æ®µäº”ï¼šæœ€ç»ˆæŠ¥å‘Šç”Ÿæˆä¸æ’ç‰ˆï¼ˆä¸»ç¬”ï¼šDeepSeek-Cï¼‰")
    process_log.append("æ­£åœ¨è¿›è¡Œç»“æ„åŒ–å†™ä½œä¸ Markdown æ’ç‰ˆ...")
    writer_prompt = PROMPTS["stage5_writer"].format(sop_json=sop_for_next, tech_summary_json=judge_for_next)
    final_report = _call_llm(roles["writer"], api_keys[roles["writer"]], model_config, writer_prompt, temperature=0.2, timeout=180)
    if final_report.startswith("Error"):
        return {"error": final_report, "log": "\n".join(process_log)}

    process_log.append("ğŸ‰ **æŠ¥å‘Šç”Ÿæˆå®Œæ¯•**ã€‚")
    return {"content": final_report, "log": "\n".join(process_log)}


