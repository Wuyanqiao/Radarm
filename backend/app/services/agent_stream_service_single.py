"""
å•æ¨¡å‹ Agent æµå¼æœåŠ¡
"""
import json
import asyncio
from typing import AsyncGenerator, Dict, Any, Optional
from backend.app.services.llm_service import LLMService


async def run_single_agent_engine_stream(
    *,
    user_query: str,
    data_context: str,
    api_keys: Dict[str, str],
    model_config: Dict[str, Any],
    primary_model: str,
    execute_callback,
    df,
    llm_service: LLMService,
) -> AsyncGenerator[str, None]:
    """
    å•æ¨¡å‹ Agent å¼•æ“ - æµå¼ç‰ˆæœ¬
    
    å®æ—¶ yield æ€è€ƒè¿‡ç¨‹ï¼š
    - "thinking: æ­£åœ¨ç”Ÿæˆä»£ç ..."
    - "thinking: æ­£åœ¨è¿è¡Œä»£ç ..."
    - "content: <å®é™…å†…å®¹>"
    - "complete: <æœ€ç»ˆç»“æœ>"
    """
    from engine_agent_single import (
        PROMPT_TEMPLATE,
        _extract_python_code,
        _auto_feedback_from_exec,
    )
    
    api_key = api_keys.get(primary_model)
    if not api_key:
        yield json.dumps({"type": "error", "content": f"æœªé…ç½® {primary_model} çš„ API Key"})
        return
    
    max_attempts = 2
    code = ""
    exec_text = ""
    exec_img = None
    plotly_json = None
    new_df = df
    
    for attempt in range(1, max_attempts + 1):
        # 1) ç”Ÿæˆä»£ç 
        yield json.dumps({
            "type": "thinking",
            "stage": "code_generation",
            "content": f"ğŸ’» æ­£åœ¨ç”Ÿæˆä»£ç ï¼ˆç¬¬ {attempt} æ¬¡å°è¯•ï¼‰..."
        })
        
        prompt = PROMPT_TEMPLATE.format(
            user_query=user_query,
            data_context=(data_context or "")
        )
        
        if attempt > 1:
            prompt = (
                prompt
                + "\n\nã€ä¸Šä¸€è½®æ‰§è¡Œå¤±è´¥åé¦ˆã€‘\n"
                + (_auto_feedback_from_exec(exec_text) or "è¯·æ ¹æ®æŠ¥é”™ä¿®å¤ä»£ç ï¼Œå¹¶ç¡®ä¿ result æœ‰è¾“å‡ºã€‚")
                + "\n\nã€ä¸Šä¸€è½®é”™è¯¯ã€‘\n"
                + (str(exec_text)[:2000] + ("...(æˆªæ–­)" if len(str(exec_text)) > 2000 else ""))
                + "\n\nè¯·è¾“å‡ºä¿®å¤åçš„å®Œæ•´ Python ä»£ç å—ï¼ˆ```python ...```ï¼‰ï¼ŒåŠ¡å¿…å¯æ‰§è¡Œã€‚"
            )
        
        code_res = ""
        async for chunk in llm_service.call_llm_stream(
            provider=primary_model,
            api_key=api_key,
            model_config=model_config,
            prompt=prompt,
        ):
            chunk_data = json.loads(chunk)
            if chunk_data.get("type") == "content":
                code_res += chunk_data.get("content", "")
                yield json.dumps({
                    "type": "content",
                    "stage": "code_generation",
                    "content": chunk_data.get("content", "")
                })
            elif chunk_data.get("type") == "complete":
                code_res = chunk_data.get("content", "")
            elif chunk_data.get("type") == "error":
                yield json.dumps({
                    "type": "error",
                    "content": f"ä»£ç ç”Ÿæˆå¤±è´¥: {chunk_data.get('content', '')}"
                })
                return
        
        code = _extract_python_code(code_res)
        
        # 2) æ‰§è¡Œä»£ç 
        yield json.dumps({
            "type": "thinking",
            "stage": "execution",
            "content": "âš™ï¸ æ­£åœ¨æ‰§è¡Œä»£ç ..."
        })
        
        result = execute_callback(code, df)
        if len(result) == 4:
            exec_text, exec_img, plotly_json, new_df = result
        else:
            exec_text, exec_img, new_df = result
            plotly_json = None
        
        has_error = isinstance(exec_text, str) and (
            exec_text.startswith("Error") or "Traceback" in exec_text or "Exception" in exec_text
        )
        
        if not has_error:
            break
        
        yield json.dumps({
            "type": "thinking",
            "stage": "execution",
            "content": f"âš ï¸ æ‰§è¡ŒæŠ¥é”™: {str(exec_text)[:100]}..."
        })
        
        if attempt < max_attempts:
            yield json.dumps({
                "type": "thinking",
                "stage": "retry",
                "content": f"ğŸ”„ å‡†å¤‡é‡è¯•ï¼ˆ{attempt + 1}/{max_attempts}ï¼‰..."
            })
    
    # 3) ç”Ÿæˆè§£é‡Š
    yield json.dumps({
        "type": "thinking",
        "stage": "explanation",
        "content": "ğŸ“ æ­£åœ¨ç”Ÿæˆè§£é‡Š..."
    })
    
    exec_text_for_explain = exec_text
    if isinstance(exec_text_for_explain, str) and len(exec_text_for_explain) > 2000:
        exec_text_for_explain = exec_text_for_explain[:2000] + "...(æˆªæ–­)"
    
    explain_prompt = (
        f"ç”¨æˆ·éœ€æ±‚ï¼š{user_query}\n"
        f"æ•°æ®æ¦‚å†µï¼š{(data_context or '')[:6000]}\n\n"
        f"ä»£ç æ‰§è¡Œç»“æœï¼š{exec_text_for_explain}\n\n"
        "è¯·ç”¨ä¸­æ–‡ç®€è¦è§£é‡Šåˆ†æç»“æœï¼Œå¹¶ç»™å‡ºä¸‹ä¸€æ­¥å»ºè®®ï¼ˆä¸è¦æœæ’°æ•°æ®ï¼‰ã€‚"
    )
    
    explain_res = ""
    async for chunk in llm_service.call_llm_stream(
        provider=primary_model,
        api_key=api_key,
        model_config=model_config,
        prompt=explain_prompt,
    ):
        chunk_data = json.loads(chunk)
        if chunk_data.get("type") == "content":
            explain_res += chunk_data.get("content", "")
            yield json.dumps({
                "type": "content",
                "stage": "explanation",
                "content": chunk_data.get("content", "")
            })
        elif chunk_data.get("type") == "complete":
            explain_res = chunk_data.get("content", "")
        elif chunk_data.get("type") == "error":
            explain_res = "è§£é‡Šç”Ÿæˆå¤±è´¥"
    
    # è¿”å›æœ€ç»ˆç»“æœ
    yield json.dumps({
        "type": "complete",
        "data": {
            "reply": explain_res if explain_res else "æ‰§è¡Œå®Œæˆï¼ˆAIæœªè¿”å›è§£é‡Šï¼‰",
            "generated_code": code,
            "execution_result": exec_text,
            "image": exec_img,
            "plotly_json": plotly_json,
            "new_df": None,
        }
    })

