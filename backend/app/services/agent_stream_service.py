"""
Agent æµå¼æœåŠ¡ - æ”¯æŒå®æ—¶è¾“å‡ºæ€è€ƒè¿‡ç¨‹
"""
import json
import asyncio
from typing import AsyncGenerator, Dict, Any, Optional
from backend.app.services.llm_service import LLMService


async def run_multi_agent_engine_stream(
    *,
    user_query: str,
    data_context: str,
    api_keys: Dict[str, str],
    model_config: Dict[str, Any],
    roles: Optional[Dict[str, str]] = None,
    execute_callback,
    df,
    llm_service: LLMService,
) -> AsyncGenerator[str, None]:
    """
    å¤šä¸“å®¶æ··åˆ Agent å¼•æ“ - æµå¼ç‰ˆæœ¬
    
    å®æ—¶ yield æ€è€ƒè¿‡ç¨‹ï¼š
    - "thinking: æ­£åœ¨è§„åˆ’..."
    - "thinking: æ­£åœ¨ç”Ÿæˆä»£ç ..."
    - "thinking: æ­£åœ¨è¿è¡Œä»£ç ..."
    - "thinking: æ­£åœ¨å®¡æ ¸..."
    - "content: <å®é™…å†…å®¹>"
    - "complete: <æœ€ç»ˆç»“æœ>"
    """
    import re
    # å¯¼å…¥å¿…è¦çš„å‡½æ•°ï¼ˆé¿å…å¾ªç¯å¯¼å…¥ï¼‰
    from engine_agent_multi import (
        PROMPTS,
        _extract_python_code,
        _extract_json,
        _auto_feedback_from_exec,
        _provider_label,
        build_semantic_hints,
    )
    
    roles = roles or {"planner": "deepseekA", "executor": "deepseekB", "verifier": "deepseekC"}
    
    # Key æ£€æŸ¥ä¸è‡ªåŠ¨è¡¥ä½
    available_keys = [k for k, v in api_keys.items() if v]
    if not available_keys:
        yield json.dumps({"type": "error", "content": "æœªé…ç½® API Key"})
        return
    
    for r in roles:
        if not api_keys.get(roles[r]):
            roles[r] = available_keys[0]
    
    iteration = 0
    max_iterations = 2
    feedback = ""
    exec_text = ""
    exec_img = None
    plotly_json = None
    new_df = df
    code = ""
    
    while iteration < max_iterations:
        iter_prefix = f"ç¬¬ {iteration + 1} è½®è¿­ä»£"
        feedback_context = f"\n\n[ä¸Šä¸€è½®åé¦ˆ]\n{feedback}\n" if feedback else ""
        
        # æ„å»ºæ•°æ®ä¸Šä¸‹æ–‡
        try:
            hints = build_semantic_hints([str(c) for c in getattr(df, "columns", [])])
            hints_text = json.dumps(hints, ensure_ascii=False, indent=2)
        except Exception:
            hints_text = "{}"
        
        enriched_data_context = (
            (data_context or "")
            + "\n\n[å­—æ®µå€™é€‰æ˜ å°„(JSON)]\n"
            + hints_text
            + "\n\nè¯·ä¼˜å…ˆä½¿ç”¨ä¸Šè¿° core_columns/qc_covariates ä¸­çš„çœŸå®åˆ—åï¼›è‹¥ç”¨æˆ·æåˆ°çš„æ¦‚å¿µæ— å¯¹åº”åˆ—ï¼Œå¿…é¡»åœ¨ç»“è®ºä¸­è¯´æ˜å¹¶ç»™å‡ºè¡¥å……å­—æ®µå»ºè®®ã€‚"
        )
        
        # 1) è§„åˆ’é˜¶æ®µ
        yield json.dumps({
            "type": "thinking",
            "stage": "planner",
            "content": f"ğŸ§  æ¶æ„å¸ˆ ({_provider_label(roles['planner'])}) æ­£åœ¨è§„åˆ’..."
        })
        
        plan = ""
        async for chunk in llm_service.call_llm_stream(
            provider=roles["planner"],
            api_key=api_keys[roles["planner"]],
            model_config=model_config,
            prompt=PROMPTS["planner"].format(
                user_query=user_query,
                data_context=enriched_data_context,
                feedback_context=feedback_context
            ),
        ):
            chunk_data = json.loads(chunk)
            if chunk_data.get("type") == "content":
                plan += chunk_data.get("content", "")
                yield json.dumps({
                    "type": "content",
                    "stage": "planner",
                    "content": chunk_data.get("content", "")
                })
            elif chunk_data.get("type") == "complete":
                plan = chunk_data.get("content", "")
            elif chunk_data.get("type") == "error":
                yield json.dumps({
                    "type": "error",
                    "content": f"è§„åˆ’é˜¶æ®µå¤±è´¥: {chunk_data.get('content', '')}"
                })
                return
        
        if plan.startswith("Error"):
            yield json.dumps({"type": "error", "content": plan})
            return
        
        yield json.dumps({
            "type": "thinking",
            "stage": "planner",
            "content": f"âœ… è§„åˆ’å®Œæˆï¼š{plan[:100]}..."
        })
        
        # 2) æ‰§è¡Œé˜¶æ®µ
        yield json.dumps({
            "type": "thinking",
            "stage": "executor",
            "content": f"ğŸ’» ç¨‹åºå‘˜ ({_provider_label(roles['executor'])}) æ­£åœ¨ç¼–ç ..."
        })
        
        code_res = ""
        async for chunk in llm_service.call_llm_stream(
            provider=roles["executor"],
            api_key=api_keys[roles["executor"]],
            model_config=model_config,
            prompt=PROMPTS["executor"].format(
                plan=plan,
                data_context=enriched_data_context
            ),
        ):
            chunk_data = json.loads(chunk)
            if chunk_data.get("type") == "content":
                code_res += chunk_data.get("content", "")
                yield json.dumps({
                    "type": "content",
                    "stage": "executor",
                    "content": chunk_data.get("content", "")
                })
            elif chunk_data.get("type") == "complete":
                code_res = chunk_data.get("content", "")
            elif chunk_data.get("type") == "error":
                yield json.dumps({
                    "type": "error",
                    "content": f"ç¼–ç é˜¶æ®µå¤±è´¥: {chunk_data.get('content', '')}"
                })
                return
        
        code = _extract_python_code(code_res)
        
        # 3) è¿è¡Œä»£ç 
        yield json.dumps({
            "type": "thinking",
            "stage": "executor",
            "content": "âš™ï¸ æ­£åœ¨è¿è¡Œä»£ç ..."
        })
        
        result = execute_callback(code, df)
        if len(result) == 4:
            exec_text, exec_img, plotly_json, new_df = result
        else:
            exec_text, exec_img, new_df = result
            plotly_json = None
        
        has_error = isinstance(exec_text, str) and (
            exec_text.startswith("Error") or "Traceback" in exec_text
        )
        
        if has_error:
            yield json.dumps({
                "type": "thinking",
                "stage": "executor",
                "content": f"âš ï¸ ä»£ç æ‰§è¡ŒæŠ¥é”™: {exec_text[:100]}..."
            })
            
            # è‡ªåŠ¨åé¦ˆ
            auto_fb = _auto_feedback_from_exec(exec_text)
            if auto_fb:
                yield json.dumps({
                    "type": "thinking",
                    "stage": "system",
                    "content": f"ğŸ¤– ç³»ç»Ÿè‡ªåŠ¨è¯Šæ–­: {auto_fb}"
                })
                feedback = auto_fb
                iteration += 1
                continue
        
        # 4) éªŒè¯é˜¶æ®µ
        yield json.dumps({
            "type": "thinking",
            "stage": "verifier",
            "content": f"âš–ï¸ è¯„å®¡å‘˜ ({_provider_label(roles['verifier'])}) æ­£åœ¨å®¡æ ¸..."
        })
        
        force_fail = "\n\nâš ï¸ ä»£ç æŠ¥é”™ï¼Œè¯·åˆ¤ FAIL å¹¶è¯´æ˜åŸå› ä¸ä¿®å¤å»ºè®®ï¼" if has_error else ""
        verify_res = ""
        async for chunk in llm_service.call_llm_stream(
            provider=roles["verifier"],
            api_key=api_keys[roles["verifier"]],
            model_config=model_config,
            prompt=PROMPTS["verifier"].format(
                plan=plan,
                code=code,
                execution_result=exec_text
            ) + force_fail,
        ):
            chunk_data = json.loads(chunk)
            if chunk_data.get("type") == "content":
                verify_res += chunk_data.get("content", "")
            elif chunk_data.get("type") == "complete":
                verify_res = chunk_data.get("content", "")
            elif chunk_data.get("type") == "error":
                yield json.dumps({
                    "type": "error",
                    "content": f"å®¡æ ¸é˜¶æ®µå¤±è´¥: {chunk_data.get('content', '')}"
                })
                return
        
        review = _extract_json(verify_res)
        if not isinstance(review, dict):
            review = {}
        
        status = str(review.get("status") or "").upper()
        reason = str(review.get("reason") or "").strip() or "è¯„å®¡æœªç»™å‡ºæ˜ç¡®åŸå› "
        suggestion = str(review.get("suggestion") or "").strip()
        final_reply = str(review.get("final_reply") or "").strip()
        
        if status not in ("PASS", "FAIL"):
            status = "FAIL" if has_error else "FAIL"
            if not suggestion:
                suggestion = "è¯„å®¡è¾“å‡ºæ— æ³•è§£æä¸ºåˆæ³• JSONã€‚è¯·ä¸¥æ ¼æŒ‰ JSON Schema è¾“å‡ºï¼Œå¹¶ä¿®å¤ä»£ç /ç»“æœä¸ºç©ºç­‰é—®é¢˜ã€‚"
        
        if status == "PASS":
            yield json.dumps({
                "type": "thinking",
                "stage": "verifier",
                "content": f"âœ… éªŒè¯é€šè¿‡: {reason}"
            })
            
            # è¿”å›æœ€ç»ˆç»“æœ
            yield json.dumps({
                "type": "complete",
                "data": {
                    "reply": final_reply if final_reply else f"### ğŸ¯ Radarm å¤šä¸“å®¶ç»“è®º\n\n**ç»“è®º**: {exec_text}\n\n**è¯„å®¡**: {reason}",
                    "generated_code": code,
                    "execution_result": exec_text,
                    "image": exec_img,
                    "plotly_json": plotly_json,
                    "new_df": None,  # DataFrame ä¸èƒ½ç›´æ¥åºåˆ—åŒ–ï¼Œéœ€è¦å•ç‹¬å¤„ç†
                }
            })
            return
        
        yield json.dumps({
            "type": "thinking",
            "stage": "verifier",
            "content": f"âŒ é©³å›: {reason}\nğŸ”„ å»ºè®®: {suggestion}"
        })
        
        feedback = suggestion
        iteration += 1
    
    # è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°
    yield json.dumps({
        "type": "complete",
        "data": {
            "reply": f"âš ï¸ è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°ã€‚æœ€åç»“æœ: {exec_text}",
            "generated_code": code,
            "execution_result": exec_text,
            "image": exec_img,
            "plotly_json": plotly_json,
            "new_df": None,
        }
    })

